{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e685266",
   "metadata": {},
   "source": [
    "# ‚è±Ô∏è Latency Analysis\n",
    "\n",
    "**Measure inference latency** for each trained model.\n",
    "\n",
    "## What this measures:\n",
    "- Time per decision step (model.predict + env.step)\n",
    "- In milliseconds\n",
    "- Important for real-time deployment feasibility\n",
    "\n",
    "‚ö†Ô∏è **Requires trained models from 02_training.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.actions import Action\n",
    "\n",
    "print(\"Imports loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a53c53",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "RUNS_DIR = \"/content/drive/MyDrive/runs\"\n",
    "LAYOUT = \"asymmetric_advantages\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "HORIZON = 400\n",
    "NUM_ACTIONS = len(Action.ALL_ACTIONS)\n",
    "\n",
    "BASELINES = [\"Baseline\", \"PPO+LLM\", \"CC_PPO\", \"SP_PPO\", \"HARL\", \"PBT_PPO\"]\n",
    "ENV_NAMES = [\"No Noise\", \"Noise\", \"Delay\", \"Combo\"]\n",
    "SEEDS = [1001, 2002, 3003, 4004, 5005]\n",
    "\n",
    "LATENCY_CSV = \"/content/drive/MyDrive/latency_results.csv\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Results will be saved to: {LATENCY_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25443e8c",
   "metadata": {},
   "source": [
    "## Environment Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCWrapper(gym.Env):\n",
    "    \"\"\"True 2-agent Overcooked wrapper.\"\"\"\n",
    "    metadata = {\"render.modes\": []}\n",
    "\n",
    "    def __init__(self, layout):\n",
    "        super().__init__()\n",
    "        mdp = OvercookedGridworld.from_layout_name(layout)\n",
    "        self.oc = OvercookedEnv.from_mdp(mdp, horizon=HORIZON)\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=o0.flatten().shape, dtype=np.float32\n",
    "        )\n",
    "        self.action_space = gym.spaces.MultiDiscrete([NUM_ACTIONS, NUM_ACTIONS])\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "        self.oc.reset()\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        return o0.flatten().astype(np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        a0, a1 = int(action[0]), int(action[1])\n",
    "        joint = [Action.ALL_ACTIONS[a0], Action.ALL_ACTIONS[a1]]\n",
    "        state, r, done, info = self.oc.step(joint)\n",
    "        o0, _ = self.oc.featurize_state_mdp(state)\n",
    "        return o0.flatten().astype(np.float32), float(r), bool(done), False, info\n",
    "\n",
    "\n",
    "class OCWrapperNoise(OCWrapper):\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        obs = (obs + np.random.normal(0, 0.01, size=obs.shape)).astype(np.float32)\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "\n",
    "class OCWrapperDelay(OCWrapper):\n",
    "    def __init__(self, layout, noise_prob=0.2, delay_penalty=0.5):\n",
    "        super().__init__(layout)\n",
    "        self.noise_prob = noise_prob\n",
    "        self.delay_penalty = delay_penalty\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        if np.random.rand() < self.noise_prob:\n",
    "            r -= self.delay_penalty\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "\n",
    "class OCWrapperCombo(OCWrapper):\n",
    "    def __init__(self, layout, noise_prob=0.2, delay_penalty=0.5):\n",
    "        super().__init__(layout)\n",
    "        self.noise_prob = noise_prob\n",
    "        self.delay_penalty = delay_penalty\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        obs = (obs + np.random.normal(0, 0.01, size=obs.shape)).astype(np.float32)\n",
    "        if np.random.rand() < self.noise_prob:\n",
    "            r -= self.delay_penalty\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "\n",
    "def make_env(env_name: str, layout: str):\n",
    "    \"\"\"Evaluation env factory.\"\"\"\n",
    "    e = env_name.lower()\n",
    "    mapping = {\n",
    "        \"no noise\": OCWrapper,\n",
    "        \"noise\": OCWrapperNoise,\n",
    "        \"delay\": OCWrapperDelay,\n",
    "        \"combo\": OCWrapperCombo,\n",
    "    }\n",
    "    return Monitor(mapping[e](layout))\n",
    "\n",
    "print(\"Environment wrappers defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980aee9a",
   "metadata": {},
   "source": [
    "## Latency Measurement Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c94895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_step_latency(agent, env, episodes=5):\n",
    "    \"\"\"\n",
    "    Measures average latency per decision step (predict + env.step),\n",
    "    in milliseconds.\n",
    "    \"\"\"\n",
    "    total_time = 0.0\n",
    "    total_steps = 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            t0 = time.perf_counter()\n",
    "\n",
    "            # Decision + transition\n",
    "            action, _ = agent.predict(obs, deterministic=True)\n",
    "            obs, r, term, trunc, _ = env.step(action)\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "\n",
    "            total_time += (t1 - t0)\n",
    "            total_steps += 1\n",
    "            done = term or trunc\n",
    "\n",
    "    if total_steps == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    # seconds ‚Üí milliseconds\n",
    "    return (total_time / total_steps) * 1000.0\n",
    "\n",
    "print(\"Measurement function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7961d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_measure_latency(baseline, env_name, seed, episodes=5):\n",
    "    \"\"\"\n",
    "    Loads a trained PPO model and measures latency.\n",
    "    \"\"\"\n",
    "    safe_base = baseline.replace(\" \", \"_\")\n",
    "    safe_env = env_name.replace(\" \", \"_\")\n",
    "    model_path = f\"{RUNS_DIR}/{safe_base}/{safe_env}/seed_{seed}/final_model.zip\"\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"[WARN] Missing model: {model_path}\")\n",
    "        return None\n",
    "\n",
    "    env = make_env(env_name, LAYOUT)\n",
    "    agent = PPO.load(model_path, env=env, device=DEVICE)\n",
    "\n",
    "    # Warmup passes (stabilize GPU / cache effects)\n",
    "    for _ in range(10):\n",
    "        obs, _ = env.reset()\n",
    "        action, _ = agent.predict(obs, deterministic=True)\n",
    "        obs, _, term, trunc, _ = env.step(action)\n",
    "        if term or trunc:\n",
    "            break\n",
    "\n",
    "    latency_ms = measure_step_latency(agent, env, episodes=episodes)\n",
    "    print(f\"{baseline} | {env_name} | seed={seed}: {latency_ms:.4f} ms/step\")\n",
    "\n",
    "    return latency_ms\n",
    "\n",
    "print(\"Load and measure function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e38020",
   "metadata": {},
   "source": [
    "## üöÄ Run Latency Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a633120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_latency_sweep():\n",
    "    \"\"\"\n",
    "    For every (baseline, env, seed), measure latency and save results.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    print(\"=== Measuring per-step latency for all models ===\")\n",
    "    for b in BASELINES:\n",
    "        for e in ENV_NAMES:\n",
    "            seed_latencies = []\n",
    "            for s in SEEDS:\n",
    "                lat = load_and_measure_latency(b, e, s, episodes=5)\n",
    "                if lat is not None and not np.isnan(lat):\n",
    "                    rows.append([b, e, s, lat])\n",
    "                    seed_latencies.append(lat)\n",
    "\n",
    "            if seed_latencies:\n",
    "                mean_lat = float(np.mean(seed_latencies))\n",
    "                std_lat = float(np.std(seed_latencies))\n",
    "                print(f\"[AGG] {b} | {e}: {mean_lat:.4f} ¬± {std_lat:.4f} ms/step\")\n",
    "                rows.append([b, e, \"mean_over_seeds\", mean_lat])\n",
    "                rows.append([b, e, \"std_over_seeds\", std_lat])\n",
    "\n",
    "    os.makedirs(os.path.dirname(LATENCY_CSV), exist_ok=True)\n",
    "    with open(LATENCY_CSV, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"baseline\", \"env\", \"seed_or_stat\", \"latency_ms\"])\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"\\nüéâ Latency sweep complete. Saved to: {LATENCY_CSV}\")\n",
    "\n",
    "# Run the sweep\n",
    "run_latency_sweep()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a35aa73",
   "metadata": {},
   "source": [
    "## üìà View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(LATENCY_CSV)\n",
    "\n",
    "# Filter to mean values only\n",
    "df_mean = df[df[\"seed_or_stat\"] == \"mean_over_seeds\"].copy()\n",
    "df_std = df[df[\"seed_or_stat\"] == \"std_over_seeds\"].copy()\n",
    "\n",
    "print(\"Mean Latency by Baseline and Environment (ms):\")\n",
    "pivot = df_mean.pivot(index=\"baseline\", columns=\"env\", values=\"latency_ms\")\n",
    "display(pivot.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d639ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Aggregate across environments\n",
    "latency_by_baseline = df_mean.groupby(\"baseline\")[\"latency_ms\"].mean().sort_values()\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.8, len(latency_by_baseline)))\n",
    "bars = ax.barh(latency_by_baseline.index, latency_by_baseline.values, color=colors)\n",
    "\n",
    "ax.set_xlabel(\"Latency (ms/step)\")\n",
    "ax.set_title(\"Mean Inference Latency by Baseline\")\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, latency_by_baseline.values):\n",
    "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "            f\"{val:.3f}\", va=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
