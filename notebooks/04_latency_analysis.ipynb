{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af676c2",
   "metadata": {},
   "source": [
    "# Latency Analysis\n",
    "\n",
    "Measure LLM inference latency per environment step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e919fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from overcooked_ai_py.mdp.actions import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LATENCY_CSV = \"/content/drive/MyDrive/latency_results.csv\"\n",
    "N_SAMPLES = 200\n",
    "WARMUP = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d032d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/gpt-neo-1.3B\",\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    ").to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bdd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def measure_latency(prompt):\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    _ = model(**enc)\n",
    "    \n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    return (time.perf_counter() - start) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"In cooperative cooking, are joint actions 'N' and 'E' helpful? Answer good or bad.\"\n",
    "\n",
    "for _ in range(WARMUP):\n",
    "    measure_latency(test_prompt)\n",
    "\n",
    "latencies = [measure_latency(test_prompt) for _ in tqdm(range(N_SAMPLES))]\n",
    "\n",
    "mean_ms = np.mean(latencies)\n",
    "std_ms = np.std(latencies)\n",
    "p50 = np.percentile(latencies, 50)\n",
    "p95 = np.percentile(latencies, 95)\n",
    "p99 = np.percentile(latencies, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"device\": DEVICE,\n",
    "    \"n_samples\": N_SAMPLES,\n",
    "    \"mean_ms\": mean_ms,\n",
    "    \"std_ms\": std_ms,\n",
    "    \"p50_ms\": p50,\n",
    "    \"p95_ms\": p95,\n",
    "    \"p99_ms\": p99\n",
    "}\n",
    "\n",
    "pd.DataFrame([results]).to_csv(LATENCY_CSV, index=False)\n",
    "\n",
    "print(f\"Mean: {mean_ms:.2f} ms\")\n",
    "print(f\"Std:  {std_ms:.2f} ms\")\n",
    "print(f\"P50:  {p50:.2f} ms\")\n",
    "print(f\"P95:  {p95:.2f} ms\")\n",
    "print(f\"P99:  {p99:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(latencies, bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].axvline(mean_ms, color=\"red\", linestyle=\"--\", label=f\"Mean: {mean_ms:.1f}ms\")\n",
    "axes[0].axvline(p95, color=\"orange\", linestyle=\"--\", label=f\"P95: {p95:.1f}ms\")\n",
    "axes[0].set_xlabel(\"Latency (ms)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(latencies, marker=\".\", alpha=0.5, linewidth=0)\n",
    "axes[1].axhline(mean_ms, color=\"red\", linestyle=\"--\")\n",
    "axes[1].set_xlabel(\"Sample\")\n",
    "axes[1].set_ylabel(\"Latency (ms)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
