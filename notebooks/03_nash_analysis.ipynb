{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe85339",
   "metadata": {},
   "source": [
    "# üìä Nash Gap Analysis\n",
    "\n",
    "**Compute Nash equilibrium gaps** to measure how close trained policies are to optimal.\n",
    "\n",
    "## What is Nash Gap?\n",
    "- Nash gap = Best Response Value - Self-Play Value\n",
    "- **Lower is better** (0 = Nash equilibrium)\n",
    "- Measures how much an agent could improve by deviating\n",
    "\n",
    "‚ö†Ô∏è **Requires trained models from 02_training.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5192b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# BR-BASED NASH ANALYSIS (PARALLEL + RESUME + CRASH-SAFE)\n",
    "# =========================================================\n",
    "\n",
    "import os, csv, random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.actions import Action\n",
    "\n",
    "print(\"Imports loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a8968",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79172748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "LAYOUT = \"asymmetric_advantages\"\n",
    "HORIZON = 400\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RUNS_DIR = \"/content/drive/MyDrive/runs\"\n",
    "\n",
    "NUM_ACTIONS = len(Action.ALL_ACTIONS)\n",
    "\n",
    "BR_RESULTS_CSV = \"/content/drive/MyDrive/br_nash_results.csv\"\n",
    "EVAL_EPISODES = 20\n",
    "BR_TRAIN_STEPS = 200_000  # Steps to train best response\n",
    "\n",
    "BASELINES = [\"Baseline\", \"PPO+LLM\", \"CC_PPO\", \"SP_PPO\", \"HARL\", \"PBT_PPO\"]\n",
    "ENV_NAMES = [\"No Noise\", \"Noise\", \"Delay\", \"Combo\"]\n",
    "SEEDS = [1001, 2002, 3003, 4004, 5005]\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Results will be saved to: {BR_RESULTS_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c233664",
   "metadata": {},
   "source": [
    "## MLAM Warmup (Fixes Pickle Crash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccbed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_mlam():\n",
    "    \"\"\"\n",
    "    Ensures MediumLevelActionManager pickle is built ONCE\n",
    "    on main process before workers spawn.\n",
    "    \"\"\"\n",
    "    print(\"Prewarming MLAM planner...\")\n",
    "    mdp = OvercookedGridworld.from_layout_name(LAYOUT)\n",
    "    env = OvercookedEnv.from_mdp(mdp, horizon=HORIZON)\n",
    "    _ = env.featurize_state_mdp(env.state)  # Forces MLAM creation\n",
    "    print(\"‚úÖ MLAM prewarm complete.\")\n",
    "\n",
    "# Run warmup\n",
    "warmup_mlam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f28f9",
   "metadata": {},
   "source": [
    "## Environment Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCWrapper(gym.Env):\n",
    "    def __init__(self, layout):\n",
    "        super().__init__()\n",
    "        mdp = OvercookedGridworld.from_layout_name(layout)\n",
    "        self.oc = OvercookedEnv.from_mdp(mdp, horizon=HORIZON)\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        self.obs_shape = o0.flatten().shape\n",
    "        self.observation_space = gym.spaces.Box(-np.inf, np.inf, self.obs_shape, dtype=np.float32)\n",
    "        self.action_space = gym.spaces.MultiDiscrete([NUM_ACTIONS, NUM_ACTIONS])\n",
    "\n",
    "    def _obs(self):\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        return o0.flatten().astype(np.float32)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "        self.oc.reset()\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        a0, a1 = int(action[0]), int(action[1])\n",
    "        joint = [Action.ALL_ACTIONS[a0], Action.ALL_ACTIONS[a1]]\n",
    "        _, r, done, info = self.oc.step(joint)\n",
    "        return self._obs(), float(r), bool(done), False, info\n",
    "\n",
    "\n",
    "class OCWrapperNoise(OCWrapper):\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        return obs + np.random.normal(0, 0.01, obs.shape), r, term, trunc, info\n",
    "\n",
    "\n",
    "class OCWrapperDelay(OCWrapper):\n",
    "    def __init__(self, layout, noise_prob=0.2, delay_penalty=0.5):\n",
    "        super().__init__(layout)\n",
    "        self.noise_prob = noise_prob\n",
    "        self.delay_penalty = delay_penalty\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        if np.random.rand() < self.noise_prob: r -= self.delay_penalty\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "\n",
    "class OCWrapperCombo(OCWrapper):\n",
    "    def __init__(self, layout, noise_prob=0.2, delay_penalty=0.5):\n",
    "        super().__init__(layout)\n",
    "        self.noise_prob = noise_prob\n",
    "        self.delay_penalty = delay_penalty\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        obs = obs + np.random.normal(0, 0.01, obs.shape)\n",
    "        if np.random.rand() < self.noise_prob: r -= self.delay_penalty\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "\n",
    "def make_env(env_name, layout):\n",
    "    m = {\n",
    "        \"no noise\": OCWrapper,\n",
    "        \"noise\": OCWrapperNoise,\n",
    "        \"delay\": OCWrapperDelay,\n",
    "        \"combo\": OCWrapperCombo,\n",
    "    }\n",
    "    return Monitor(m[env_name.lower()](layout))\n",
    "\n",
    "print(\"Environment wrappers defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f90a01",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    set_random_seed(seed)\n",
    "\n",
    "\n",
    "def eval_joint_policy(model, env, episodes=EVAL_EPISODES):\n",
    "    \"\"\"Evaluate self-play performance.\"\"\"\n",
    "    scores = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        ep = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            a, _ = model.predict(obs, deterministic=True)\n",
    "            obs, r, term, trunc, _ = env.step(a)\n",
    "            done = term or trunc\n",
    "            ep += r\n",
    "        scores.append(ep)\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "print(\"Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba166df",
   "metadata": {},
   "source": [
    "## Best Response Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25953119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OvercookedBREnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Best Response environment:\n",
    "    - One agent trains while opponent uses fixed policy\n",
    "    \"\"\"\n",
    "    def __init__(self, env_name, layout, opponent_model, agent_idx):\n",
    "        super().__init__()\n",
    "        self.agent_idx = agent_idx\n",
    "        self.opponent_idx = 1 - agent_idx\n",
    "        self.opponent_model = opponent_model\n",
    "\n",
    "        self.base_env = make_env(env_name, layout).env\n",
    "        self.observation_space = self.base_env.observation_space\n",
    "        self.action_space = gym.spaces.Discrete(NUM_ACTIONS)\n",
    "        self._last_obs = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            set_global_seed(seed)\n",
    "        obs, info = self.base_env.reset(seed=seed)\n",
    "        self._last_obs = obs\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        a_self = int(action)\n",
    "        opp_joint, _ = self.opponent_model.predict(self._last_obs, deterministic=True)\n",
    "        a_opp = int(opp_joint[self.opponent_idx])\n",
    "\n",
    "        joint = np.zeros(2, dtype=np.int64)\n",
    "        joint[self.agent_idx] = a_self\n",
    "        joint[self.opponent_idx] = a_opp\n",
    "\n",
    "        obs, r, term, trunc, info = self.base_env.step(joint)\n",
    "        self._last_obs = obs\n",
    "        return obs, float(r), bool(term), bool(trunc), info\n",
    "\n",
    "print(\"BR environment defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788dcec",
   "metadata": {},
   "source": [
    "## Best Response Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_response(env_name, layout, opponent_model, agent_idx, seed):\n",
    "    \"\"\"Train a best response agent against fixed opponent.\"\"\"\n",
    "    set_global_seed(seed)\n",
    "    br_env = Monitor(OvercookedBREnv(env_name, layout, opponent_model, agent_idx))\n",
    "\n",
    "    br = PPO(\"MlpPolicy\", br_env,\n",
    "             n_steps=2048, batch_size=2048,\n",
    "             learning_rate=3e-4, gamma=0.99,\n",
    "             verbose=0, device=DEVICE, seed=seed)\n",
    "\n",
    "    br.learn(total_timesteps=BR_TRAIN_STEPS)\n",
    "\n",
    "    # Evaluate BR\n",
    "    scores = []\n",
    "    for _ in range(EVAL_EPISODES):\n",
    "        obs, _ = br_env.reset()\n",
    "        ep = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            a, _ = br.predict(obs, deterministic=True)\n",
    "            obs, r, term, trunc, _ = br_env.step(a)\n",
    "            done = term or trunc\n",
    "            ep += r\n",
    "        scores.append(ep)\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "print(\"BR training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43535ba2",
   "metadata": {},
   "source": [
    "## Nash Gap Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affed8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nash_gap_for_model(baseline, env_name, seed):\n",
    "    \"\"\"Compute Nash gap for a single trained model.\"\"\"\n",
    "    safe_base = baseline.replace(\" \", \"_\")\n",
    "    safe_env = env_name.replace(\" \", \"_\")\n",
    "    model_path = f\"{RUNS_DIR}/{safe_base}/{safe_env}/seed_{seed}/final_model.zip\"\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"‚ùå Missing: {model_path}\")\n",
    "        return None\n",
    "\n",
    "    model = PPO.load(model_path, device=DEVICE)\n",
    "\n",
    "    # Self-play value\n",
    "    env_self = make_env(env_name, LAYOUT)\n",
    "    v_self_m, v_self_s = eval_joint_policy(model, env_self)\n",
    "\n",
    "    # Best response value\n",
    "    v_br_m, v_br_s = train_best_response(env_name, LAYOUT, model, 0, seed+999)\n",
    "\n",
    "    delta = v_br_m - v_self_m\n",
    "    print(f\"‚úÖ {baseline}|{env_name}|{seed}: V_self={v_self_m:.2f}, V_BR={v_br_m:.2f}, Œî={delta:.2f}\")\n",
    "\n",
    "    return {\n",
    "        \"baseline\": baseline,\n",
    "        \"env\": env_name,\n",
    "        \"seed\": seed,\n",
    "        \"V_self_mean\": v_self_m,\n",
    "        \"V_self_std\": v_self_s,\n",
    "        \"V_BR_mean\": v_br_m,\n",
    "        \"V_BR_std\": v_br_s,\n",
    "        \"delta\": delta,\n",
    "    }\n",
    "\n",
    "print(\"Nash gap function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6878532",
   "metadata": {},
   "source": [
    "## Resume Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57648fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_completed_set():\n",
    "    \"\"\"Load already-computed results to skip.\"\"\"\n",
    "    done = set()\n",
    "    if os.path.exists(BR_RESULTS_CSV):\n",
    "        with open(BR_RESULTS_CSV) as f:\n",
    "            next(f)  # Skip header\n",
    "            for row in csv.reader(f):\n",
    "                done.add((row[0], row[1], row[2]))\n",
    "    return done\n",
    "\n",
    "print(\"Resume logic defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc53837",
   "metadata": {},
   "source": [
    "## üöÄ Run Nash Gap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ff17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CSV\n",
    "if not os.path.exists(BR_RESULTS_CSV):\n",
    "    with open(BR_RESULTS_CSV, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"baseline\", \"env\", \"seed\",\n",
    "            \"V_self_mean\", \"V_self_std\",\n",
    "            \"V_BR_mean\", \"V_BR_std\", \"delta\"\n",
    "        ])\n",
    "    print(f\"Created {BR_RESULTS_CSV}\")\n",
    "\n",
    "completed = load_completed_set()\n",
    "print(f\"Already completed: {len(completed)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda49baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build job list (skip completed)\n",
    "jobs = [(b, e, s) for b in BASELINES for e in ENV_NAMES for s in SEEDS\n",
    "        if (b, e, str(s)) not in completed]\n",
    "\n",
    "print(f\"Remaining jobs: {len(jobs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a71854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(job):\n",
    "    \"\"\"Compute and save result for one job.\"\"\"\n",
    "    res = compute_nash_gap_for_model(*job)\n",
    "    if res is None:\n",
    "        return None\n",
    "\n",
    "    with open(BR_RESULTS_CSV, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            res[\"baseline\"], res[\"env\"], res[\"seed\"],\n",
    "            round(res[\"V_self_mean\"], 2),\n",
    "            round(res[\"V_self_std\"], 2),\n",
    "            round(res[\"V_BR_mean\"], 2),\n",
    "            round(res[\"V_BR_std\"], 2),\n",
    "            round(res[\"delta\"], 2),\n",
    "        ])\n",
    "    return res\n",
    "\n",
    "# Run parallel analysis\n",
    "print(\"üöÄ Running Nash gap analysis...\")\n",
    "Parallel(n_jobs=6)(delayed(wrapper)(job) for job in jobs)\n",
    "\n",
    "print(f\"\\nüéâ DONE! Results saved to: {BR_RESULTS_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c305f7",
   "metadata": {},
   "source": [
    "## üìà View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(BR_RESULTS_CSV)\n",
    "print(f\"Total results: {len(df)}\")\n",
    "\n",
    "# Aggregate by baseline and env\n",
    "agg = df.groupby([\"baseline\", \"env\"]).agg({\n",
    "    \"V_self_mean\": \"mean\",\n",
    "    \"V_BR_mean\": \"mean\",\n",
    "    \"delta\": [\"mean\", \"std\"]\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nüìä Nash Gap Summary:\")\n",
    "display(agg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
