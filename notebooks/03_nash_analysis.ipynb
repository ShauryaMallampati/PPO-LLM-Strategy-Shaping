{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e072835c",
   "metadata": {},
   "source": [
    "# 03 - Nash Equilibrium Analysis\n",
    "\n",
    "Best-Response based Nash gap analysis for trained models.\n",
    "\n",
    "**Computes:**\n",
    "- Self-play value (V_self)\n",
    "- Best-response value (V_BR)\n",
    "- Nash gap (delta = V_BR - V_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# BR-BASED NASH ANALYSIS (PARALLEL + RESUME + CRASH-SAFE)\n",
    "# FIXED MLAM PLANNER BUG\n",
    "# =========================================================\n",
    "\n",
    "import os, csv, random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.actions import Action\n",
    "\n",
    "# ---- MATCH TRAINING CONFIG ----\n",
    "LAYOUT = \"asymmetric_advantages\"\n",
    "HORIZON = 400\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RUNS_DIR = \"/content/drive/MyDrive/runs\"\n",
    "\n",
    "NUM_ACTIONS = len(Action.ALL_ACTIONS)\n",
    "\n",
    "BR_RESULTS_CSV = \"/content/drive/MyDrive/br_nash_results.csv\"\n",
    "EVAL_EPISODES = 20\n",
    "BR_TRAIN_STEPS = 200_000\n",
    "\n",
    "BASELINES = [\"Baseline\", \"PPO+LLM\", \"CC_PPO\", \"SP_PPO\", \"HARL\", \"PBT_PPO\"]\n",
    "ENV_NAMES = [\"No Noise\", \"Noise\", \"Delay\", \"Combo\"]\n",
    "SEEDS = [1001, 2002, 3003, 4004, 5005]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# MLAM PREWARM (fixes pickle crash)\n",
    "# =========================================================\n",
    "def warmup_mlam():\n",
    "    \"\"\"\n",
    "    Ensures MediumLevelActionManager pickle is built ONCE\n",
    "    on main process before workers spawn.\n",
    "    \"\"\"\n",
    "    print(\"Prewarming MLAM planner...\")\n",
    "    mdp = OvercookedGridworld.from_layout_name(LAYOUT)\n",
    "    env = OvercookedEnv.from_mdp(mdp, horizon=HORIZON)\n",
    "\n",
    "    # This call forces MLAM creation\n",
    "    _ = env.featurize_state_mdp(env.state)\n",
    "\n",
    "    print(\"MLAM prewarm complete.\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ENVIRONMENT WRAPPERS (NEVER trigger MLAM in workers)\n",
    "# =========================================================\n",
    "class OCWrapper(gym.Env):\n",
    "\n",
    "    def __init__(self, layout):\n",
    "        super().__init__()\n",
    "        mdp = OvercookedGridworld.from_layout_name(layout)\n",
    "        # MLAM already exists now due to warmup\n",
    "        self.oc = OvercookedEnv.from_mdp(mdp, horizon=HORIZON)\n",
    "\n",
    "        # Precompute obs_dim so step() never calls featurize\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        self.obs_shape = o0.flatten().shape\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(-np.inf, np.inf, self.obs_shape, dtype=np.float32)\n",
    "        self.action_space = gym.spaces.MultiDiscrete([NUM_ACTIONS, NUM_ACTIONS])\n",
    "\n",
    "    def _obs(self):\n",
    "        # Directly read state vector, guaranteed safe post-warmup\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        return o0.flatten().astype(np.float32)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "        self.oc.reset()\n",
    "        return self._obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        a0, a1 = int(action[0]), int(action[1])\n",
    "        joint = [Action.ALL_ACTIONS[a0], Action.ALL_ACTIONS[a1]]\n",
    "        _, r, done, info = self.oc.step(joint)\n",
    "        return self._obs(), float(r), bool(done), False, info\n",
    "\n",
    "\n",
    "class OCWrapperNoise(OCWrapper):\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        return obs + np.random.normal(0,0.01, obs.shape), r, term, trunc, info\n",
    "\n",
    "\n",
    "class OCWrapperDelay(OCWrapper):\n",
    "    def __init__(self, layout, noise_prob=0.2, delay_penalty=0.5):\n",
    "        super().__init__(layout)\n",
    "        self.noise_prob = noise_prob\n",
    "        self.delay_penalty = delay_penalty\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        if np.random.rand() < self.noise_prob: r -= self.delay_penalty\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "\n",
    "class OCWrapperCombo(OCWrapper):\n",
    "    def __init__(self, layout, noise_prob=0.2, delay_penalty=0.5):\n",
    "        super().__init__(layout)\n",
    "        self.noise_prob = noise_prob\n",
    "        self.delay_penalty = delay_penalty\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        obs = obs + np.random.normal(0,0.01, obs.shape)\n",
    "        if np.random.rand() < self.noise_prob: r -= self.delay_penalty\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "\n",
    "def make_env(env_name, layout):\n",
    "    m = {\n",
    "        \"no noise\": OCWrapper,\n",
    "        \"noise\": OCWrapperNoise,\n",
    "        \"delay\": OCWrapperDelay,\n",
    "        \"combo\": OCWrapperCombo,\n",
    "    }\n",
    "    return Monitor(m[env_name.lower()](layout))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# SELF-PLAY EVAL\n",
    "# =========================================================\n",
    "\n",
    "def set_global_seed(seed):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    set_random_seed(seed)\n",
    "\n",
    "def eval_joint_policy(model, env, episodes=EVAL_EPISODES):\n",
    "    scores = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        ep = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            a, _ = model.predict(obs, deterministic=True)\n",
    "            obs, r, term, trunc, _ = env.step(a)\n",
    "            done = term or trunc\n",
    "            ep += r\n",
    "        scores.append(ep)\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# BR ENVIRONMENT\n",
    "# =========================================================\n",
    "class OvercookedBREnv(gym.Env):\n",
    "    def __init__(self, env_name, layout, opponent_model, agent_idx):\n",
    "        super().__init__()\n",
    "        self.agent_idx = agent_idx\n",
    "        self.opponent_idx = 1 - agent_idx\n",
    "        self.opponent_model = opponent_model\n",
    "\n",
    "        self.base_env = make_env(env_name, layout).env\n",
    "        self.observation_space = self.base_env.observation_space\n",
    "        self.action_space = gym.spaces.Discrete(NUM_ACTIONS)\n",
    "        self._last_obs = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            set_global_seed(seed)\n",
    "        obs, info = self.base_env.reset(seed=seed)\n",
    "        self._last_obs = obs\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        a_self = int(action)\n",
    "        opp_joint, _ = self.opponent_model.predict(self._last_obs, deterministic=True)\n",
    "        a_opp = int(opp_joint[self.opponent_idx])\n",
    "\n",
    "        joint = np.zeros(2, dtype=np.int64)\n",
    "        joint[self.agent_idx] = a_self\n",
    "        joint[self.opponent_idx] = a_opp\n",
    "\n",
    "        obs, r, term, trunc, info = self.base_env.step(joint)\n",
    "        self._last_obs = obs\n",
    "        return obs, float(r), bool(term), bool(trunc), info\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# BEST RESPONSE TRAINING\n",
    "# =========================================================\n",
    "def train_best_response(env_name, layout, opponent_model, agent_idx, seed):\n",
    "    set_global_seed(seed)\n",
    "    br_env = Monitor(OvercookedBREnv(env_name, layout, opponent_model, agent_idx))\n",
    "\n",
    "    br = PPO(\"MlpPolicy\", br_env,\n",
    "             n_steps=2048, batch_size=2048,\n",
    "             learning_rate=3e-4, gamma=0.99,\n",
    "             verbose=0, device=DEVICE, seed=seed)\n",
    "\n",
    "    br.learn(total_timesteps=BR_TRAIN_STEPS)\n",
    "\n",
    "    scores = []\n",
    "    for _ in range(EVAL_EPISODES):\n",
    "        obs, _ = br_env.reset()\n",
    "        ep = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            a, _ = br.predict(obs, deterministic=True)\n",
    "            obs, r, term, trunc, _ = br_env.step(a)\n",
    "            done = term or trunc\n",
    "            ep += r\n",
    "        scores.append(ep)\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# COMPUTE NASH GAP\n",
    "# =========================================================\n",
    "def compute_nash_gap_for_model(baseline, env_name, seed):\n",
    "\n",
    "    safe_base = baseline.replace(\" \", \"_\")\n",
    "    safe_env = env_name.replace(\" \", \"_\")\n",
    "    model_path = f\"{RUNS_DIR}/{safe_base}/{safe_env}/seed_{seed}/final_model.zip\"\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"Missing:\", model_path)\n",
    "        return None\n",
    "\n",
    "    model = PPO.load(model_path, device=DEVICE)\n",
    "\n",
    "    env_self = make_env(env_name, LAYOUT)\n",
    "    v_self_m, v_self_s = eval_joint_policy(model, env_self)\n",
    "\n",
    "    v_br_m, v_br_s = train_best_response(env_name, LAYOUT, model, 0, seed+999)\n",
    "\n",
    "    return {\n",
    "        \"baseline\": baseline,\n",
    "        \"env\": env_name,\n",
    "        \"seed\": seed,\n",
    "        \"V_self_mean\": v_self_m,\n",
    "        \"V_self_std\": v_self_s,\n",
    "        \"V_BR_mean\": v_br_m,\n",
    "        \"V_BR_std\": v_br_s,\n",
    "        \"delta\": v_br_m - v_self_m,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# CSV RESUME LOGIC\n",
    "# =========================================================\n",
    "def load_completed_set():\n",
    "    done = set()\n",
    "    if os.path.exists(BR_RESULTS_CSV):\n",
    "        with open(BR_RESULTS_CSV) as f:\n",
    "            next(f)\n",
    "            for row in csv.reader(f):\n",
    "                done.add((row[0], row[1], row[2]))\n",
    "    return done\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# MAIN (PARALLEL + RESUME)\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    warmup_mlam()   # <-- THIS FIXES THE ERROR\n",
    "\n",
    "    if not os.path.exists(BR_RESULTS_CSV):\n",
    "        with open(BR_RESULTS_CSV, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                \"baseline\",\"env\",\"seed\",\n",
    "                \"V_self_mean\",\"V_self_std\",\n",
    "                \"V_BR_mean\",\"V_BR_std\",\"delta\"\n",
    "            ])\n",
    "\n",
    "    completed = load_completed_set()\n",
    "\n",
    "    jobs = [(b, e, s) for b in BASELINES for e in ENV_NAMES for s in SEEDS\n",
    "            if (b, e, str(s)) not in completed]\n",
    "\n",
    "    print(\"Remaining jobs:\", len(jobs))\n",
    "\n",
    "    def wrapper(job):\n",
    "        res = compute_nash_gap_for_model(*job)\n",
    "        if res is None: return None\n",
    "\n",
    "        with open(BR_RESULTS_CSV, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                res[\"baseline\"], res[\"env\"], res[\"seed\"],\n",
    "                round(res[\"V_self_mean\"],2),\n",
    "                round(res[\"V_self_std\"],2),\n",
    "                round(res[\"V_BR_mean\"],2),\n",
    "                round(res[\"V_BR_std\"],2),\n",
    "                round(res[\"delta\"],2),\n",
    "            ])\n",
    "        print(\"Saved:\", job)\n",
    "        return res\n",
    "\n",
    "    Parallel(n_jobs=6)(delayed(wrapper)(job) for job in jobs)\n",
    "\n",
    "    print(\"DONE. Saved to:\", BR_RESULTS_CSV)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
