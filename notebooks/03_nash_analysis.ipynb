{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa3ff74",
   "metadata": {},
   "source": [
    "# Nash Analysis\n",
    "\n",
    "Best-response Nash gap analysis with MLAM warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac710e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.actions import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYOUT = \"asymmetric_advantages\"\n",
    "HORIZON = 400\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEEDS = [1001, 2002, 3003, 4004, 5005]\n",
    "\n",
    "RUNS_DIR = \"/content/drive/MyDrive/runs\"\n",
    "BR_RESULTS_CSV = \"/content/drive/MyDrive/br_nash_results.csv\"\n",
    "\n",
    "BR_TRAIN_STEPS = 200_000\n",
    "BR_EVAL_EPISODES = 20\n",
    "MLAM_WARMUP_STEPS = 50_000\n",
    "\n",
    "BASELINES = [\"Baseline\", \"PPO+LLM\", \"CC_PPO\", \"SP_PPO\", \"HARL\", \"PBT_PPO\"]\n",
    "ENV_NAMES = [\"No Noise\", \"Noise\", \"Delay\", \"Combo\"]\n",
    "NUM_ACTIONS = len(Action.ALL_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bfbf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAgentBREnv(gym.Env):\n",
    "    \"\"\"Environment for training best response against fixed partner.\"\"\"\n",
    "    metadata = {\"render.modes\": []}\n",
    "\n",
    "    def __init__(self, layout, fixed_policy, agent_idx):\n",
    "        super().__init__()\n",
    "        mdp = OvercookedGridworld.from_layout_name(layout)\n",
    "        self.oc = OvercookedEnv.from_mdp(mdp, horizon=HORIZON)\n",
    "        self.fixed = fixed_policy\n",
    "        self.idx = agent_idx\n",
    "        \n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=o0.flatten().shape, dtype=np.float32\n",
    "        )\n",
    "        self.action_space = gym.spaces.Discrete(NUM_ACTIONS)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.oc.reset()\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        return o0.flatten().astype(np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        obs = self._get_obs()\n",
    "        dummy_joint = np.array([0, 0])\n",
    "        partner_action, _ = self.fixed.predict(np.concatenate([obs, obs]), deterministic=True)\n",
    "        partner_a = int(partner_action[1 - self.idx])\n",
    "        \n",
    "        if self.idx == 0:\n",
    "            joint = [Action.ALL_ACTIONS[int(action)], Action.ALL_ACTIONS[partner_a]]\n",
    "        else:\n",
    "            joint = [Action.ALL_ACTIONS[partner_a], Action.ALL_ACTIONS[int(action)]]\n",
    "        \n",
    "        state, r, done, info = self.oc.step(joint)\n",
    "        o0, _ = self.oc.featurize_state_mdp(state)\n",
    "        return o0.flatten().astype(np.float32), float(r), bool(done), False, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        return o0.flatten().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e599ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_policy(baseline, env_name, seed):\n",
    "    safe_base = baseline.replace(\" \", \"_\")\n",
    "    safe_env = env_name.replace(\" \", \"_\")\n",
    "    path = os.path.join(RUNS_DIR, safe_base, safe_env, f\"seed_{seed}\", \"final_model.zip\")\n",
    "    if os.path.exists(path):\n",
    "        return PPO.load(path, device=DEVICE)\n",
    "    return None\n",
    "\n",
    "\n",
    "def evaluate_joint(policy, env, episodes=20):\n",
    "    scores = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_r = 0.0\n",
    "        while not done:\n",
    "            action, _ = policy.predict(obs, deterministic=True)\n",
    "            obs, r, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            ep_r += r\n",
    "        scores.append(ep_r)\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def train_best_response(fixed_policy, agent_idx, steps=BR_TRAIN_STEPS):\n",
    "    env = Monitor(SingleAgentBREnv(LAYOUT, fixed_policy, agent_idx))\n",
    "    br = PPO(\"MlpPolicy\", env, verbose=0, device=DEVICE)\n",
    "    br.learn(total_timesteps=steps)\n",
    "    return br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCWrapper(gym.Env):\n",
    "    metadata = {\"render.modes\": []}\n",
    "\n",
    "    def __init__(self, layout):\n",
    "        super().__init__()\n",
    "        mdp = OvercookedGridworld.from_layout_name(layout)\n",
    "        self.oc = OvercookedEnv.from_mdp(mdp, horizon=HORIZON)\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=o0.flatten().shape, dtype=np.float32\n",
    "        )\n",
    "        self.action_space = gym.spaces.MultiDiscrete([NUM_ACTIONS, NUM_ACTIONS])\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.oc.reset()\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        return o0.flatten().astype(np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        a0, a1 = int(action[0]), int(action[1])\n",
    "        joint = [Action.ALL_ACTIONS[a0], Action.ALL_ACTIONS[a1]]\n",
    "        state, r, done, info = self.oc.step(joint)\n",
    "        o0, _ = self.oc.featurize_state_mdp(state)\n",
    "        return o0.flatten().astype(np.float32), float(r), bool(done), False, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nash_gap(baseline, env_name, seed):\n",
    "    policy = load_policy(baseline, env_name, seed)\n",
    "    if policy is None:\n",
    "        return None\n",
    "    \n",
    "    eval_env = OCWrapper(LAYOUT)\n",
    "    v_joint = evaluate_joint(policy, eval_env, BR_EVAL_EPISODES)\n",
    "    \n",
    "    br0 = train_best_response(policy, agent_idx=0)\n",
    "    br1 = train_best_response(policy, agent_idx=1)\n",
    "    \n",
    "    br0_env = SingleAgentBREnv(LAYOUT, policy, 0)\n",
    "    br1_env = SingleAgentBREnv(LAYOUT, policy, 1)\n",
    "    \n",
    "    v_br0 = np.mean([sum_episode(br0, br0_env) for _ in range(BR_EVAL_EPISODES)])\n",
    "    v_br1 = np.mean([sum_episode(br1, br1_env) for _ in range(BR_EVAL_EPISODES)])\n",
    "    \n",
    "    nash_gap = (v_br0 - v_joint) + (v_br1 - v_joint)\n",
    "    return nash_gap\n",
    "\n",
    "\n",
    "def sum_episode(policy, env):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total = 0.0\n",
    "    while not done:\n",
    "        action, _ = policy.predict(obs, deterministic=True)\n",
    "        obs, r, term, trunc, _ = env.step(action)\n",
    "        done = term or trunc\n",
    "        total += r\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69421533",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for baseline in tqdm(BASELINES, desc=\"Baselines\"):\n",
    "    for env_name in ENV_NAMES:\n",
    "        for seed in SEEDS:\n",
    "            gap = compute_nash_gap(baseline, env_name, seed)\n",
    "            if gap is not None:\n",
    "                results.append({\n",
    "                    \"baseline\": baseline,\n",
    "                    \"env\": env_name,\n",
    "                    \"seed\": seed,\n",
    "                    \"nash_gap\": gap\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(BR_RESULTS_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc886602",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = df.groupby([\"baseline\", \"env\"]).agg(\n",
    "    nash_gap_mean=(\"nash_gap\", \"mean\"),\n",
    "    nash_gap_std=(\"nash_gap\", \"std\")\n",
    ").reset_index()\n",
    "\n",
    "print(agg.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3cbe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = agg.pivot(index=\"baseline\", columns=\"env\", values=\"nash_gap_mean\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "pivot.plot(kind=\"bar\", ax=ax)\n",
    "ax.set_ylabel(\"Nash Gap\")\n",
    "ax.set_title(\"Nash Gap by Baseline and Environment\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
