{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0ab8d0",
   "metadata": {},
   "source": [
    "# 02 - Training Script\n",
    "\n",
    "Main training script for PPO+LLM Strategy Shaping experiments.\n",
    "\n",
    "**Features:**\n",
    "- True 2-agent Overcooked (no [a, a] mirroring)\n",
    "- Runs 20 jobs in parallel on A100\n",
    "- Includes StopTrainingOnMaxSteps callback\n",
    "- PPO+LLM stops at 600k steps, others at 1M steps\n",
    "\n",
    "**Baselines:** Baseline, PPO+LLM, CC_PPO, SP_PPO, HARL, PBT_PPO\n",
    "\n",
    "**Environment Regimes:** No Noise, Noise, Delay, Combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# TRAINING SCRIPT (PARALLEL + FIXED STOPPING + 2-AGENT ENV)\n",
    "# - True 2-agent Overcooked (no [a, a] mirroring)\n",
    "# - Runs 20 jobs at once on A100\n",
    "# - Includes StopTrainingOnMaxSteps callback\n",
    "# - PPO+LLM stops at 600k steps\n",
    "# =========================================================\n",
    "import os, re, gc, json, time, csv, random, math, pickle\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch, pandas as pd\n",
    "import torch.nn as nn\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.actions import Action\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "LAYOUT = \"asymmetric_advantages\"  # or \"burger\" etc.\n",
    "HORIZON = 400\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEEDS = [1001, 2002, 3003, 4004, 5005]\n",
    "\n",
    "CHECKPOINT_EVERY_STEPS = 50_000\n",
    "LOG_EVERY_STEPS = 2_048\n",
    "EVAL_EPISODES = 10\n",
    "\n",
    "RESULTS_CSV = \"/content/drive/MyDrive/results_combined_new.csv\"\n",
    "RUNS_DIR = \"/content/drive/MyDrive/runs\"\n",
    "LLM_BONUS = 0.2\n",
    "\n",
    "BASELINE_STEPS = {\n",
    "    \"Baseline\": 1_000_000,\n",
    "    \"CC_PPO\":   1_000_000,\n",
    "    \"SP_PPO\":   1_000_000,\n",
    "    \"HARL\":     1_000_000,\n",
    "    \"PPO+LLM\":  600_000,\n",
    "    \"PBT_PPO\":  1_000_000,\n",
    "}\n",
    "\n",
    "NUM_ACTIONS = len(Action.ALL_ACTIONS)\n",
    "\n",
    "# ==========================================\n",
    "# 2. LLM SETUP (LAZY LOAD)\n",
    "# ==========================================\n",
    "_GLOBAL_LLM = None\n",
    "_GLOBAL_TOK = None\n",
    "\n",
    "def get_llm():\n",
    "    global _GLOBAL_LLM, _GLOBAL_TOK\n",
    "    if _GLOBAL_LLM is None:\n",
    "        try:\n",
    "            _GLOBAL_TOK = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "            _GLOBAL_LLM = AutoModelForCausalLM.from_pretrained(\n",
    "                \"EleutherAI/gpt-neo-1.3B\",\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            ).to(DEVICE).eval()\n",
    "        except Exception:\n",
    "            _GLOBAL_LLM = False\n",
    "    return _GLOBAL_LLM, _GLOBAL_TOK\n",
    "\n",
    "@torch.no_grad()\n",
    "def is_good(prompt: str) -> bool:\n",
    "    llm, tok = get_llm()\n",
    "    if not llm:\n",
    "        return False\n",
    "    try:\n",
    "        enc = tok(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        logits = llm(**enc).logits[0, -1]\n",
    "        def idx(tok_str):\n",
    "            ids = tok.encode(tok_str, add_special_tokens=False)\n",
    "            return ids[0] if ids else None\n",
    "        gi, bi = idx(\" good\"), idx(\" bad\")\n",
    "        if gi is None or bi is None:\n",
    "            return False\n",
    "        return logits[gi].item() > logits[bi].item()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ==========================================\n",
    "# 3. ENVIRONMENT WRAPPERS (TRUE 2-AGENT)\n",
    "# ==========================================\n",
    "class OCWrapper(gym.Env):\n",
    "    \"\"\"\n",
    "    True 2-agent Overcooked wrapper:\n",
    "    - observation: global featurized state (flattened)\n",
    "    - action_space: MultiDiscrete([NUM_ACTIONS, NUM_ACTIONS])\n",
    "    - reward: shared team reward\n",
    "    \"\"\"\n",
    "    metadata = {\"render.modes\": []}\n",
    "\n",
    "    def __init__(self, layout):\n",
    "        super().__init__()\n",
    "        mdp = OvercookedGridworld.from_layout_name(layout)\n",
    "        self.oc = OvercookedEnv.from_mdp(mdp, horizon=HORIZON)\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=o0.flatten().shape,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.action_space = gym.spaces.MultiDiscrete([NUM_ACTIONS, NUM_ACTIONS])\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "        self.oc.reset()\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        return o0.flatten().astype(np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # action is [a0, a1]\n",
    "        a0, a1 = int(action[0]), int(action[1])\n",
    "        joint = [Action.ALL_ACTIONS[a0], Action.ALL_ACTIONS[a1]]\n",
    "        state, r, done, info = self.oc.step(joint)\n",
    "        o0, _ = self.oc.featurize_state_mdp(state)\n",
    "        return o0.flatten().astype(np.float32), float(r), bool(done), False, info\n",
    "\n",
    "class OCWrapperNoise(OCWrapper):\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        obs = (obs + np.random.normal(0, 0.01, size=obs.shape)).astype(np.float32)\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "class OCWrapperDelay(OCWrapper):\n",
    "    def __init__(self, layout, noise_prob=0.2, delay_penalty=0.5):\n",
    "        super().__init__(layout)\n",
    "        self.noise_prob = noise_prob\n",
    "        self.delay_penalty = delay_penalty\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        if np.random.rand() < self.noise_prob:\n",
    "            r -= self.delay_penalty\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "class OCWrapperCombo(OCWrapper):\n",
    "    def __init__(self, layout, noise_prob=0.2, delay_penalty=0.5):\n",
    "        super().__init__(layout)\n",
    "        self.noise_prob = noise_prob\n",
    "        self.delay_penalty = delay_penalty\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        obs = (obs + np.random.normal(0, 0.01, size=obs.shape)).astype(np.float32)\n",
    "        if np.random.rand() < self.noise_prob:\n",
    "            r -= self.delay_penalty\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "class OCWrapperLLM(OCWrapper):\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        a0, a1 = int(action[0]), int(action[1])\n",
    "        act0 = Action.ACTION_TO_CHAR[Action.ALL_ACTIONS[a0]]\n",
    "        act1 = Action.ACTION_TO_CHAR[Action.ALL_ACTIONS[a1]]\n",
    "        prompt = f\"In cooperative cooking, are joint actions '{act0}' and '{act1}' helpful? Answer good or bad.\"\n",
    "        if is_good(prompt):\n",
    "            r += LLM_BONUS\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "class _LLMNoise(OCWrapperLLM, OCWrapperNoise):\n",
    "    pass\n",
    "\n",
    "class _LLMDelay(OCWrapperLLM, OCWrapperDelay):\n",
    "    pass\n",
    "\n",
    "class _LLMCombo(OCWrapperLLM, OCWrapperCombo):\n",
    "    pass\n",
    "\n",
    "class HARL(OCWrapper):\n",
    "    def step(self, action):\n",
    "        obs, reward, term, trunc, info = super().step(action)\n",
    "        orders_remaining = info.get(\"orders_remaining\", 0)\n",
    "        if orders_remaining == 0:\n",
    "            reward += 1.0\n",
    "        elif orders_remaining < 3:\n",
    "            reward += 0.5\n",
    "        return obs, reward, term, trunc, info\n",
    "\n",
    "class _HARLNoise(HARL, OCWrapperNoise):\n",
    "    pass\n",
    "\n",
    "class _HARLDelay(HARL, OCWrapperDelay):\n",
    "    pass\n",
    "\n",
    "class _HARLCombo(HARL, OCWrapperCombo):\n",
    "    pass\n",
    "\n",
    "class SP_PPO(OCWrapper):\n",
    "    def __init__(self, layout):\n",
    "        super().__init__(layout)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FACTORIES\n",
    "# ---------------------------------------------------------\n",
    "def make_env(env_name, layout):\n",
    "    e = env_name.lower()\n",
    "    mapping = {\n",
    "        \"no noise\": OCWrapper,\n",
    "        \"noise\": OCWrapperNoise,\n",
    "        \"delay\": OCWrapperDelay,\n",
    "        \"combo\": OCWrapperCombo,\n",
    "    }\n",
    "    return Monitor(mapping[e](layout))\n",
    "\n",
    "def make_train_env(baseline, layout, env_name):\n",
    "    b, e = baseline.lower(), env_name.lower()\n",
    "    if b == \"ppo+llm\":\n",
    "        cls = {\n",
    "            \"no noise\": OCWrapperLLM,\n",
    "            \"noise\": _LLMNoise,\n",
    "            \"delay\": _LLMDelay,\n",
    "            \"combo\": _LLMCombo,\n",
    "        }[e]\n",
    "        return Monitor(cls(layout))\n",
    "    if b == \"harl\":\n",
    "        cls = {\n",
    "            \"no noise\": HARL,\n",
    "            \"noise\": _HARLNoise,\n",
    "            \"delay\": _HARLDelay,\n",
    "            \"combo\": _HARLCombo,\n",
    "        }[e]\n",
    "        return Monitor(cls(layout))\n",
    "    if b == \"sp_ppo\":\n",
    "        return Monitor(SP_PPO(layout))\n",
    "    # Baseline, CC_PPO, PBT_PPO\n",
    "    return make_env(env_name, layout)\n",
    "\n",
    "# ==========================================\n",
    "# 4. CUSTOM CALLBACKS\n",
    "# ==========================================\n",
    "class StopTrainingOnMaxSteps(BaseCallback):\n",
    "    \"\"\"\n",
    "    Hard stop when num_timesteps reaches max_steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_steps: int, verbose: int = 0):\n",
    "        super().__init__(verbose=verbose)\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps >= self.max_steps:\n",
    "            if self.verbose > 0:\n",
    "                print(\n",
    "                    f\"Stopping training because the number of steps \"\n",
    "                    f\"{self.num_timesteps} reached the limit {self.max_steps}\"\n",
    "                )\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "class PBTCallback(BaseCallback):\n",
    "    def __init__(self, check_freq=5000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.patience = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            if len(self.model.ep_info_buffer) > 0:\n",
    "                mean_reward = np.mean([ep_info[\"r\"] for ep_info in self.model.ep_info_buffer])\n",
    "            else:\n",
    "                mean_reward = -np.inf\n",
    "            if mean_reward <= self.best_mean_reward + 0.5:\n",
    "                self.patience += 1\n",
    "            else:\n",
    "                self.best_mean_reward = mean_reward\n",
    "                self.patience = 0\n",
    "            if self.patience >= 2:\n",
    "                old_lr = self.model.learning_rate\n",
    "                new_lr = old_lr * np.random.choice([0.8, 1.2])\n",
    "                self.model.learning_rate = new_lr\n",
    "                for pg in self.model.policy.optimizer.param_groups:\n",
    "                    pg[\"lr\"] = new_lr\n",
    "                self.patience = 0\n",
    "        return True\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN TRAINING LOOP\n",
    "# ==========================================\n",
    "def set_global_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    set_random_seed(seed)\n",
    "\n",
    "def evaluate(agent, env, episodes=EVAL_EPISODES):\n",
    "    scores = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_r = 0.0\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs, deterministic=True)\n",
    "            obs, r, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            ep_r += float(r)\n",
    "        scores.append(ep_r)\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "def train_one_run(baseline, env_name, seed, steps_total):\n",
    "    set_global_seed(seed)\n",
    "    label = f\"{baseline}|{env_name}|{seed}\"\n",
    "    safe_base, safe_env = baseline.replace(\" \", \"_\"), env_name.replace(\" \", \"_\")\n",
    "    run_dir = os.path.join(RUNS_DIR, safe_base, safe_env, f\"seed_{seed}\")\n",
    "    ckpt_dir = os.path.join(run_dir, \"checkpoints\")\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    train_env = make_train_env(baseline, LAYOUT, env_name)\n",
    "    eval_env = make_env(env_name, LAYOUT)\n",
    "\n",
    "    final_path = os.path.join(run_dir, \"final_model.zip\")\n",
    "    if os.path.exists(final_path):\n",
    "        # already done\n",
    "        return\n",
    "\n",
    "    ckpt = None\n",
    "    if os.path.isdir(ckpt_dir):\n",
    "        cands = [f for f in os.listdir(ckpt_dir) if f.startswith(\"ppo_\") and f.endswith(\".zip\")]\n",
    "        if cands:\n",
    "            cands.sort(key=lambda x: int(re.search(r\"(\\d+)\", x).group(1)), reverse=True)\n",
    "            ckpt = os.path.join(ckpt_dir, cands[0])\n",
    "\n",
    "    if ckpt:\n",
    "        agent = PPO.load(ckpt, env=train_env, device=DEVICE, verbose=1)\n",
    "        reset_flag = False\n",
    "    else:\n",
    "        agent = PPO(\n",
    "            \"MlpPolicy\",\n",
    "            train_env,\n",
    "            n_steps=2048,\n",
    "            batch_size=2048,\n",
    "            learning_rate=3e-4,\n",
    "            gamma=0.99,\n",
    "            verbose=1,\n",
    "            seed=seed,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        reset_flag = True\n",
    "\n",
    "    callbacks = [\n",
    "        CheckpointCallback(save_freq=CHECKPOINT_EVERY_STEPS, save_path=ckpt_dir, name_prefix=\"ppo\"),\n",
    "        StopTrainingOnMaxSteps(max_steps=steps_total, verbose=1),\n",
    "    ]\n",
    "    if baseline == \"PBT_PPO\":\n",
    "        callbacks.append(PBTCallback(check_freq=10000))\n",
    "\n",
    "    t0 = time.time()\n",
    "    agent.learn(\n",
    "        total_timesteps=steps_total,\n",
    "        callback=callbacks,\n",
    "        reset_num_timesteps=reset_flag,\n",
    "    )\n",
    "    minutes = (time.time() - t0) / 60.0\n",
    "\n",
    "    agent.save(final_path)\n",
    "    with open(os.path.join(run_dir, \"meta.json\"), \"w\") as f:\n",
    "        json.dump(\n",
    "            {\"baseline\": baseline, \"env\": env_name, \"seed\": seed, \"steps\": steps_total},\n",
    "            f,\n",
    "        )\n",
    "\n",
    "    mean, std = evaluate(agent, eval_env, episodes=EVAL_EPISODES)\n",
    "    try:\n",
    "        with open(RESULTS_CSV, \"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(\n",
    "                [baseline, env_name, seed, \"final\", round(mean, 4), round(std, 4), round(minutes, 3)]\n",
    "            )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"âœ… [{label}] FINISH in {minutes:.2f} min | eval {mean:.2f}Â±{std:.2f}\")\n",
    "    return (baseline, env_name, seed, mean)\n",
    "\n",
    "# ==========================================\n",
    "# 6. PARALLEL EXECUTION\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(RESULTS_CSV):\n",
    "        with open(RESULTS_CSV, \"w\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(\n",
    "                [\"baseline\", \"env\", \"seed\", \"phase\", \"mean_return\", \"std_dev\", \"train_minutes\"]\n",
    "            )\n",
    "\n",
    "    baselines = [\"Baseline\", \"PPO+LLM\", \"CC_PPO\", \"SP_PPO\", \"HARL\", \"PBT_PPO\"]\n",
    "    env_names = [\"No Noise\", \"Noise\", \"Delay\", \"Combo\"]\n",
    "\n",
    "    all_jobs = []\n",
    "    for b in baselines:\n",
    "        steps = BASELINE_STEPS.get(b, 1_000_000)\n",
    "        for e in env_names:\n",
    "            for s in SEEDS:\n",
    "                all_jobs.append((b, e, s, steps))\n",
    "\n",
    "    print(f\"ðŸš€ Launching 20 parallel jobs... logs will interleave below\")\n",
    "    Parallel(n_jobs=20)(\n",
    "        delayed(train_one_run)(b, e, s, steps)\n",
    "        for b, e, s, steps in all_jobs\n",
    "    )\n",
    "    print(\"\\nðŸŽ‰ ALL RUNS COMPLETE.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
