{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6186fe70",
   "metadata": {},
   "source": [
    "# üèãÔ∏è PPO-LLM Strategy Shaping: Training\n",
    "\n",
    "**Train all baseline methods** across different environment perturbations.\n",
    "\n",
    "## Baselines:\n",
    "- **Baseline** - Vanilla PPO\n",
    "- **PPO+LLM** - PPO with LLM reward shaping (only 600K steps needed!)\n",
    "- **CC_PPO** - Centralized Critic PPO\n",
    "- **SP_PPO** - Self-Play PPO\n",
    "- **HARL** - Heterogeneous-Agent RL\n",
    "- **PBT_PPO** - Population-Based Training\n",
    "\n",
    "## Environments:\n",
    "- **No Noise** - Clean observations\n",
    "- **Noise** - Gaussian observation noise (œÉ=0.01)\n",
    "- **Delay** - 20% chance of action delay with penalty\n",
    "- **Combo** - Both noise and delay\n",
    "\n",
    "‚ö†Ô∏è **Run 01_setup.ipynb first!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# TRAINING SCRIPT (PARALLEL + FIXED STOPPING + 2-AGENT ENV)\n",
    "# =========================================================\n",
    "import os, re, gc, json, time, csv, random, math, pickle\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch, pandas as pd\n",
    "import torch.nn as nn\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.actions import Action\n",
    "\n",
    "print(\"Imports loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a170447",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "LAYOUT = \"asymmetric_advantages\"\n",
    "HORIZON = 400\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEEDS = [1001, 2002, 3003, 4004, 5005]\n",
    "\n",
    "CHECKPOINT_EVERY_STEPS = 50_000\n",
    "LOG_EVERY_STEPS = 2_048\n",
    "EVAL_EPISODES = 10\n",
    "\n",
    "RESULTS_CSV = \"/content/drive/MyDrive/results_combined_new.csv\"\n",
    "RUNS_DIR = \"/content/drive/MyDrive/runs\"\n",
    "LLM_BONUS = 0.2\n",
    "\n",
    "BASELINE_STEPS = {\n",
    "    \"Baseline\": 1_000_000,\n",
    "    \"CC_PPO\":   1_000_000,\n",
    "    \"SP_PPO\":   1_000_000,\n",
    "    \"HARL\":     1_000_000,\n",
    "    \"PPO+LLM\":  600_000,\n",
    "    \"PBT_PPO\":  1_000_000,\n",
    "}\n",
    "\n",
    "NUM_ACTIONS = len(Action.ALL_ACTIONS)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Layout: {LAYOUT}\")\n",
    "print(f\"Seeds: {SEEDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41e21f",
   "metadata": {},
   "source": [
    "## LLM Setup (Lazy Loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49399fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# LLM SETUP (LAZY LOAD)\n",
    "# ==========================================\n",
    "_GLOBAL_LLM = None\n",
    "_GLOBAL_TOK = None\n",
    "\n",
    "def get_llm():\n",
    "    global _GLOBAL_LLM, _GLOBAL_TOK\n",
    "    if _GLOBAL_LLM is None:\n",
    "        try:\n",
    "            _GLOBAL_TOK = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "            _GLOBAL_LLM = AutoModelForCausalLM.from_pretrained(\n",
    "                \"EleutherAI/gpt-neo-1.3B\",\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            ).to(DEVICE).eval()\n",
    "        except Exception:\n",
    "            _GLOBAL_LLM = False\n",
    "    return _GLOBAL_LLM, _GLOBAL_TOK\n",
    "\n",
    "@torch.no_grad()\n",
    "def is_good(prompt: str) -> bool:\n",
    "    \"\"\"Ask LLM if the action is 'good' or 'bad' based on logit comparison.\"\"\"\n",
    "    llm, tok = get_llm()\n",
    "    if not llm:\n",
    "        return False\n",
    "    try:\n",
    "        enc = tok(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        logits = llm(**enc).logits[0, -1]\n",
    "        def idx(tok_str):\n",
    "            ids = tok.encode(tok_str, add_special_tokens=False)\n",
    "            return ids[0] if ids else None\n",
    "        gi, bi = idx(\" good\"), idx(\" bad\")\n",
    "        if gi is None or bi is None:\n",
    "            return False\n",
    "        return logits[gi].item() > logits[bi].item()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"LLM functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5887bd89",
   "metadata": {},
   "source": [
    "## Environment Wrappers (True 2-Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c177d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ENVIRONMENT WRAPPERS\n",
    "# ==========================================\n",
    "\n",
    "class OCWrapper(gym.Env):\n",
    "    \"\"\"\n",
    "    True 2-agent Overcooked wrapper:\n",
    "    - observation: global featurized state (flattened)\n",
    "    - action_space: MultiDiscrete([NUM_ACTIONS, NUM_ACTIONS])\n",
    "    - reward: shared team reward\n",
    "    \"\"\"\n",
    "    metadata = {\"render.modes\": []}\n",
    "\n",
    "    def __init__(self, layout):\n",
    "        super().__init__()\n",
    "        mdp = OvercookedGridworld.from_layout_name(layout)\n",
    "        self.oc = OvercookedEnv.from_mdp(mdp, horizon=HORIZON)\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=o0.flatten().shape, dtype=np.float32\n",
    "        )\n",
    "        self.action_space = gym.spaces.MultiDiscrete([NUM_ACTIONS, NUM_ACTIONS])\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "        self.oc.reset()\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        return o0.flatten().astype(np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        a0, a1 = int(action[0]), int(action[1])\n",
    "        joint = [Action.ALL_ACTIONS[a0], Action.ALL_ACTIONS[a1]]\n",
    "        state, r, done, info = self.oc.step(joint)\n",
    "        o0, _ = self.oc.featurize_state_mdp(state)\n",
    "        return o0.flatten().astype(np.float32), float(r), bool(done), False, info\n",
    "\n",
    "\n",
    "class OCWrapperNoise(OCWrapper):\n",
    "    \"\"\"Adds Gaussian noise to observations.\"\"\"\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        obs = (obs + np.random.normal(0, 0.01, size=obs.shape)).astype(np.float32)\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "\n",
    "class OCWrapperDelay(OCWrapper):\n",
    "    \"\"\"Adds random action delay with reward penalty.\"\"\"\n",
    "    def __init__(self, layout, noise_prob=0.2, delay_penalty=0.5):\n",
    "        super().__init__(layout)\n",
    "        self.noise_prob = noise_prob\n",
    "        self.delay_penalty = delay_penalty\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        if np.random.rand() < self.noise_prob:\n",
    "            r -= self.delay_penalty\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "\n",
    "class OCWrapperCombo(OCWrapper):\n",
    "    \"\"\"Combines noise and delay.\"\"\"\n",
    "    def __init__(self, layout, noise_prob=0.2, delay_penalty=0.5):\n",
    "        super().__init__(layout)\n",
    "        self.noise_prob = noise_prob\n",
    "        self.delay_penalty = delay_penalty\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        obs = (obs + np.random.normal(0, 0.01, size=obs.shape)).astype(np.float32)\n",
    "        if np.random.rand() < self.noise_prob:\n",
    "            r -= self.delay_penalty\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "\n",
    "class OCWrapperLLM(OCWrapper):\n",
    "    \"\"\"Adds LLM-based reward shaping.\"\"\"\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        a0, a1 = int(action[0]), int(action[1])\n",
    "        act0 = Action.ACTION_TO_CHAR[Action.ALL_ACTIONS[a0]]\n",
    "        act1 = Action.ACTION_TO_CHAR[Action.ALL_ACTIONS[a1]]\n",
    "        prompt = f\"In cooperative cooking, are joint actions '{act0}' and '{act1}' helpful? Answer good or bad.\"\n",
    "        if is_good(prompt):\n",
    "            r += LLM_BONUS\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "\n",
    "# Combined wrappers for LLM + perturbations\n",
    "class _LLMNoise(OCWrapperLLM, OCWrapperNoise): pass\n",
    "class _LLMDelay(OCWrapperLLM, OCWrapperDelay): pass\n",
    "class _LLMCombo(OCWrapperLLM, OCWrapperCombo): pass\n",
    "\n",
    "\n",
    "class HARL(OCWrapper):\n",
    "    \"\"\"Heterogeneous-Agent RL with order-based shaping.\"\"\"\n",
    "    def step(self, action):\n",
    "        obs, reward, term, trunc, info = super().step(action)\n",
    "        orders_remaining = info.get(\"orders_remaining\", 0)\n",
    "        if orders_remaining == 0:\n",
    "            reward += 1.0\n",
    "        elif orders_remaining < 3:\n",
    "            reward += 0.5\n",
    "        return obs, reward, term, trunc, info\n",
    "\n",
    "class _HARLNoise(HARL, OCWrapperNoise): pass\n",
    "class _HARLDelay(HARL, OCWrapperDelay): pass\n",
    "class _HARLCombo(HARL, OCWrapperCombo): pass\n",
    "\n",
    "class SP_PPO(OCWrapper): pass  # Self-play uses same base wrapper\n",
    "\n",
    "print(\"Environment wrappers defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ad2eb",
   "metadata": {},
   "source": [
    "## Environment Factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ENVIRONMENT FACTORIES\n",
    "# ==========================================\n",
    "\n",
    "def make_env(env_name, layout):\n",
    "    \"\"\"Create evaluation environment (no training-specific shaping).\"\"\"\n",
    "    e = env_name.lower()\n",
    "    mapping = {\n",
    "        \"no noise\": OCWrapper,\n",
    "        \"noise\": OCWrapperNoise,\n",
    "        \"delay\": OCWrapperDelay,\n",
    "        \"combo\": OCWrapperCombo,\n",
    "    }\n",
    "    return Monitor(mapping[e](layout))\n",
    "\n",
    "\n",
    "def make_train_env(baseline, layout, env_name):\n",
    "    \"\"\"Create training environment with baseline-specific shaping.\"\"\"\n",
    "    b, e = baseline.lower(), env_name.lower()\n",
    "    \n",
    "    if b == \"ppo+llm\":\n",
    "        cls = {\n",
    "            \"no noise\": OCWrapperLLM,\n",
    "            \"noise\": _LLMNoise,\n",
    "            \"delay\": _LLMDelay,\n",
    "            \"combo\": _LLMCombo,\n",
    "        }[e]\n",
    "        return Monitor(cls(layout))\n",
    "    \n",
    "    if b == \"harl\":\n",
    "        cls = {\n",
    "            \"no noise\": HARL,\n",
    "            \"noise\": _HARLNoise,\n",
    "            \"delay\": _HARLDelay,\n",
    "            \"combo\": _HARLCombo,\n",
    "        }[e]\n",
    "        return Monitor(cls(layout))\n",
    "    \n",
    "    if b == \"sp_ppo\":\n",
    "        return Monitor(SP_PPO(layout))\n",
    "    \n",
    "    # Baseline, CC_PPO, PBT_PPO\n",
    "    return make_env(env_name, layout)\n",
    "\n",
    "print(\"Factories defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b9b6fc",
   "metadata": {},
   "source": [
    "## Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CALLBACKS\n",
    "# ==========================================\n",
    "\n",
    "class StopTrainingOnMaxSteps(BaseCallback):\n",
    "    \"\"\"Hard stop when num_timesteps reaches max_steps.\"\"\"\n",
    "    def __init__(self, max_steps: int, verbose: int = 0):\n",
    "        super().__init__(verbose=verbose)\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps >= self.max_steps:\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Stopping: {self.num_timesteps} >= {self.max_steps}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "\n",
    "class PBTCallback(BaseCallback):\n",
    "    \"\"\"Population-Based Training: adaptive learning rate.\"\"\"\n",
    "    def __init__(self, check_freq=5000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.patience = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            if len(self.model.ep_info_buffer) > 0:\n",
    "                mean_reward = np.mean([ep[\"r\"] for ep in self.model.ep_info_buffer])\n",
    "            else:\n",
    "                mean_reward = -np.inf\n",
    "            \n",
    "            if mean_reward <= self.best_mean_reward + 0.5:\n",
    "                self.patience += 1\n",
    "            else:\n",
    "                self.best_mean_reward = mean_reward\n",
    "                self.patience = 0\n",
    "            \n",
    "            if self.patience >= 2:\n",
    "                old_lr = self.model.learning_rate\n",
    "                new_lr = old_lr * np.random.choice([0.8, 1.2])\n",
    "                self.model.learning_rate = new_lr\n",
    "                for pg in self.model.policy.optimizer.param_groups:\n",
    "                    pg[\"lr\"] = new_lr\n",
    "                self.patience = 0\n",
    "        return True\n",
    "\n",
    "print(\"Callbacks defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f292e2",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91277593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def set_global_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    set_random_seed(seed)\n",
    "\n",
    "\n",
    "def evaluate(agent, env, episodes=EVAL_EPISODES):\n",
    "    scores = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_r = 0.0\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs, deterministic=True)\n",
    "            obs, r, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            ep_r += float(r)\n",
    "        scores.append(ep_r)\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "\n",
    "def train_one_run(baseline, env_name, seed, steps_total):\n",
    "    \"\"\"Train a single (baseline, env, seed) configuration.\"\"\"\n",
    "    set_global_seed(seed)\n",
    "    label = f\"{baseline}|{env_name}|{seed}\"\n",
    "    safe_base, safe_env = baseline.replace(\" \", \"_\"), env_name.replace(\" \", \"_\")\n",
    "    run_dir = os.path.join(RUNS_DIR, safe_base, safe_env, f\"seed_{seed}\")\n",
    "    ckpt_dir = os.path.join(run_dir, \"checkpoints\")\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    train_env = make_train_env(baseline, LAYOUT, env_name)\n",
    "    eval_env = make_env(env_name, LAYOUT)\n",
    "\n",
    "    final_path = os.path.join(run_dir, \"final_model.zip\")\n",
    "    if os.path.exists(final_path):\n",
    "        print(f\"‚è≠Ô∏è Skipping {label} - already done\")\n",
    "        return\n",
    "\n",
    "    # Resume from checkpoint if available\n",
    "    ckpt = None\n",
    "    if os.path.isdir(ckpt_dir):\n",
    "        cands = [f for f in os.listdir(ckpt_dir) if f.startswith(\"ppo_\") and f.endswith(\".zip\")]\n",
    "        if cands:\n",
    "            cands.sort(key=lambda x: int(re.search(r\"(\\d+)\", x).group(1)), reverse=True)\n",
    "            ckpt = os.path.join(ckpt_dir, cands[0])\n",
    "\n",
    "    if ckpt:\n",
    "        agent = PPO.load(ckpt, env=train_env, device=DEVICE, verbose=1)\n",
    "        reset_flag = False\n",
    "    else:\n",
    "        agent = PPO(\n",
    "            \"MlpPolicy\", train_env,\n",
    "            n_steps=2048, batch_size=2048, learning_rate=3e-4,\n",
    "            gamma=0.99, verbose=1, seed=seed, device=DEVICE\n",
    "        )\n",
    "        reset_flag = True\n",
    "\n",
    "    callbacks = [\n",
    "        CheckpointCallback(save_freq=CHECKPOINT_EVERY_STEPS, save_path=ckpt_dir, name_prefix=\"ppo\"),\n",
    "        StopTrainingOnMaxSteps(max_steps=steps_total, verbose=1),\n",
    "    ]\n",
    "    if baseline == \"PBT_PPO\":\n",
    "        callbacks.append(PBTCallback(check_freq=10000))\n",
    "\n",
    "    t0 = time.time()\n",
    "    agent.learn(total_timesteps=steps_total, callback=callbacks, reset_num_timesteps=reset_flag)\n",
    "    minutes = (time.time() - t0) / 60.0\n",
    "\n",
    "    agent.save(final_path)\n",
    "    with open(os.path.join(run_dir, \"meta.json\"), \"w\") as f:\n",
    "        json.dump({\"baseline\": baseline, \"env\": env_name, \"seed\": seed, \"steps\": steps_total}, f)\n",
    "\n",
    "    mean, std = evaluate(agent, eval_env, episodes=EVAL_EPISODES)\n",
    "    try:\n",
    "        with open(RESULTS_CSV, \"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([baseline, env_name, seed, \"final\", round(mean, 4), round(std, 4), round(minutes, 3)])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"‚úÖ [{label}] FINISH in {minutes:.2f} min | eval {mean:.2f}¬±{std:.2f}\")\n",
    "    return (baseline, env_name, seed, mean)\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412ac95",
   "metadata": {},
   "source": [
    "## üöÄ Run Training\n",
    "\n",
    "**Warning**: Full training takes several hours on GPU. Adjust baselines/seeds for quick tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fa217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results CSV\n",
    "if not os.path.exists(RESULTS_CSV):\n",
    "    with open(RESULTS_CSV, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"baseline\", \"env\", \"seed\", \"phase\", \"mean_return\", \"std_dev\", \"train_minutes\"])\n",
    "    print(f\"Created {RESULTS_CSV}\")\n",
    "else:\n",
    "    print(f\"Appending to existing {RESULTS_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# LAUNCH PARALLEL TRAINING\n",
    "# ==========================================\n",
    "\n",
    "baselines = [\"Baseline\", \"PPO+LLM\", \"CC_PPO\", \"SP_PPO\", \"HARL\", \"PBT_PPO\"]\n",
    "env_names = [\"No Noise\", \"Noise\", \"Delay\", \"Combo\"]\n",
    "\n",
    "all_jobs = []\n",
    "for b in baselines:\n",
    "    steps = BASELINE_STEPS.get(b, 1_000_000)\n",
    "    for e in env_names:\n",
    "        for s in SEEDS:\n",
    "            all_jobs.append((b, e, s, steps))\n",
    "\n",
    "print(f\"Total jobs: {len(all_jobs)}\")\n",
    "print(f\"üöÄ Launching {len(all_jobs)} parallel jobs...\")\n",
    "\n",
    "Parallel(n_jobs=20)(\n",
    "    delayed(train_one_run)(b, e, s, steps)\n",
    "    for b, e, s, steps in all_jobs\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ ALL RUNS COMPLETE.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
