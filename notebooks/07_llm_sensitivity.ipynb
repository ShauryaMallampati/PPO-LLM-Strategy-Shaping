{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3399e471",
   "metadata": {},
   "source": [
    "# 07 - LLM Sensitivity Analysis (GPT-Neo 125M)\n",
    "\n",
    "Trains PPO+LLM with GPT-Neo 125M (smaller model) to study LLM size sensitivity.\n",
    "\n",
    "**Output:**\n",
    "- Models saved to `/content/drive/MyDrive/runs_gptneo_125M/`\n",
    "- Results saved to `/content/drive/MyDrive/results_gptneo_125M.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee650db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# PPO+LLM TRAINING WITH GPT-NEO 125M (ALL FOUR ENV REGIMES)\n",
    "# - True 2-agent Overcooked (no [a, a] mirroring)\n",
    "# - Uses LLM shaping\n",
    "# - Baseline: PPO+LLM only, over No Noise / Noise / Delay / Combo\n",
    "# - Saves to /content/drive/MyDrive/runs_gptneo_125M and results_gptneo_125M.csv\n",
    "# =========================================================\n",
    "\n",
    "import os, re, json, time, csv, random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv, OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.actions import Action\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "LAYOUT = \"asymmetric_advantages\"\n",
    "HORIZON = 400\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEEDS = [1001, 2002, 3003, 4004, 5005]\n",
    "\n",
    "CHECKPOINT_EVERY_STEPS = 50_000\n",
    "EVAL_EPISODES = 10\n",
    "\n",
    "# Tag this run as using GPT-Neo 125M\n",
    "MODEL_TAG = \"gptneo_125M\"\n",
    "LLM_MODEL_NAME = \"EleutherAI/gpt-neo-125M\"\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive\"\n",
    "RUNS_DIR = os.path.join(BASE_DIR, f\"runs_{MODEL_TAG}\")\n",
    "RESULTS_CSV = os.path.join(BASE_DIR, f\"results_{MODEL_TAG}.csv\")\n",
    "\n",
    "LLM_BONUS = 0.2\n",
    "\n",
    "BASELINE_STEPS = {\n",
    "    \"PPO+LLM\": 600_000,\n",
    "}\n",
    "\n",
    "NUM_ACTIONS = len(Action.ALL_ACTIONS)\n",
    "\n",
    "os.makedirs(RUNS_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# 2. LLM SETUP (LAZY LOAD)\n",
    "# ==========================================\n",
    "_GLOBAL_LLM = None\n",
    "_GLOBAL_TOK = None\n",
    "\n",
    "def get_llm():\n",
    "    global _GLOBAL_LLM, _GLOBAL_TOK\n",
    "    if _GLOBAL_LLM is None:\n",
    "        try:\n",
    "            print(f\"Loading LLM: {LLM_MODEL_NAME} on {DEVICE}\")\n",
    "            _GLOBAL_TOK = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "            _GLOBAL_LLM = AutoModelForCausalLM.from_pretrained(\n",
    "                LLM_MODEL_NAME,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            ).to(DEVICE).eval()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load LLM {LLM_MODEL_NAME}: {e}\")\n",
    "            _GLOBAL_LLM = False\n",
    "    return _GLOBAL_LLM, _GLOBAL_TOK\n",
    "\n",
    "@torch.no_grad()\n",
    "def is_good(prompt: str) -> bool:\n",
    "    llm, tok = get_llm()\n",
    "    if not llm:\n",
    "        return False\n",
    "    try:\n",
    "        enc = tok(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        logits = llm(**enc).logits[0, -1]\n",
    "\n",
    "        def idx(tok_str):\n",
    "            ids = tok.encode(tok_str, add_special_tokens=False)\n",
    "            return ids[0] if ids else None\n",
    "\n",
    "        gi, bi = idx(\" good\"), idx(\" bad\")\n",
    "        if gi is None or bi is None:\n",
    "            return False\n",
    "        return logits[gi].item() > logits[bi].item()\n",
    "    except Exception as e:\n",
    "        print(f\"LLM inference failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# ==========================================\n",
    "# 3. ENVIRONMENT WRAPPERS (TRUE 2-AGENT)\n",
    "# ==========================================\n",
    "class OCWrapper(gym.Env):\n",
    "    \"\"\"\n",
    "    True 2-agent Overcooked wrapper:\n",
    "    - observation: global featurized state (flattened)\n",
    "    - action_space: MultiDiscrete([NUM_ACTIONS, NUM_ACTIONS])\n",
    "    - reward: shared team reward\n",
    "    \"\"\"\n",
    "    metadata = {\"render.modes\": []}\n",
    "\n",
    "    def __init__(self, layout):\n",
    "        super().__init__()\n",
    "        mdp = OvercookedGridworld.from_layout_name(layout)\n",
    "        self.oc = OvercookedEnv.from_mdp(mdp, horizon=HORIZON)\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=o0.flatten().shape,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.action_space = gym.spaces.MultiDiscrete([NUM_ACTIONS, NUM_ACTIONS])\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "        self.oc.reset()\n",
    "        o0, _ = self.oc.featurize_state_mdp(self.oc.state)\n",
    "        return o0.flatten().astype(np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        a0, a1 = int(action[0]), int(action[1])\n",
    "        joint = [Action.ALL_ACTIONS[a0], Action.ALL_ACTIONS[a1]]\n",
    "        state, r, done, info = self.oc.step(joint)\n",
    "        o0, _ = self.oc.featurize_state_mdp(state)\n",
    "        return o0.flatten().astype(np.float32), float(r), bool(done), False, info\n",
    "\n",
    "class OCWrapperNoise(OCWrapper):\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        obs = (obs + np.random.normal(0, 0.01, size=obs.shape)).astype(np.float32)\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "class OCWrapperDelay(OCWrapper):\n",
    "    def __init__(self, layout, noise_prob=0.2, delay_penalty=0.5):\n",
    "        super().__init__(layout)\n",
    "        self.noise_prob = noise_prob\n",
    "        self.delay_penalty = delay_penalty\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        if np.random.rand() < self.noise_prob:\n",
    "            r -= self.delay_penalty\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "class OCWrapperCombo(OCWrapper):\n",
    "    def __init__(self, layout, noise_prob=0.2, delay_penalty=0.5):\n",
    "        super().__init__(layout)\n",
    "        self.noise_prob = noise_prob\n",
    "        self.delay_penalty = delay_penalty\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        obs = (obs + np.random.normal(0, 0.01, size=obs.shape)).astype(np.float32)\n",
    "        if np.random.rand() < self.noise_prob:\n",
    "            r -= self.delay_penalty\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "class OCWrapperLLM(OCWrapper):\n",
    "    def step(self, action):\n",
    "        obs, r, term, trunc, info = super().step(action)\n",
    "        a0, a1 = int(action[0]), int(action[1])\n",
    "        act0 = Action.ACTION_TO_CHAR[Action.ALL_ACTIONS[a0]]\n",
    "        act1 = Action.ACTION_TO_CHAR[Action.ALL_ACTIONS[a1]]\n",
    "        prompt = (\n",
    "            f\"In cooperative cooking, are joint actions '{act0}' and '{act1}' helpful? \"\n",
    "            f\"Answer good or bad.\"\n",
    "        )\n",
    "        if is_good(prompt):\n",
    "            r += LLM_BONUS\n",
    "        return obs, r, term, trunc, info\n",
    "\n",
    "class _LLMNoise(OCWrapperLLM, OCWrapperNoise):\n",
    "    pass\n",
    "\n",
    "class _LLMDelay(OCWrapperLLM, OCWrapperDelay):\n",
    "    pass\n",
    "\n",
    "class _LLMCombo(OCWrapperLLM, OCWrapperCombo):\n",
    "    pass\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FACTORIES\n",
    "# ---------------------------------------------------------\n",
    "def make_env(env_name, layout):\n",
    "    e = env_name.lower()\n",
    "    mapping = {\n",
    "        \"no noise\": OCWrapper,\n",
    "        \"noise\": OCWrapperNoise,\n",
    "        \"delay\": OCWrapperDelay,\n",
    "        \"combo\": OCWrapperCombo,\n",
    "    }\n",
    "    return Monitor(mapping[e](layout))\n",
    "\n",
    "def make_train_env(baseline, layout, env_name):\n",
    "    # Only PPO+LLM is supported in this script\n",
    "    assert baseline.lower() == \"ppo+llm\"\n",
    "    e = env_name.lower()\n",
    "    cls = {\n",
    "        \"no noise\": OCWrapperLLM,\n",
    "        \"noise\": _LLMNoise,\n",
    "        \"delay\": _LLMDelay,\n",
    "        \"combo\": _LLMCombo,\n",
    "    }[e]\n",
    "    return Monitor(cls(layout))\n",
    "\n",
    "# ==========================================\n",
    "# 4. CALLBACKS\n",
    "# ==========================================\n",
    "class StopTrainingOnMaxSteps(BaseCallback):\n",
    "    def __init__(self, max_steps: int, verbose: int = 0):\n",
    "        super().__init__(verbose=verbose)\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps >= self.max_steps:\n",
    "            if self.verbose > 0:\n",
    "                print(\n",
    "                    f\"Stopping training because the number of steps \"\n",
    "                    f\"{self.num_timesteps} reached the limit {self.max_steps}\"\n",
    "                )\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "# ==========================================\n",
    "# 5. UTILS\n",
    "# ==========================================\n",
    "def set_global_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    set_random_seed(seed)\n",
    "\n",
    "def evaluate(agent, env, episodes=EVAL_EPISODES):\n",
    "    scores = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_r = 0.0\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs, deterministic=True)\n",
    "            obs, r, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            ep_r += float(r)\n",
    "        scores.append(ep_r)\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "# ==========================================\n",
    "# 6. MAIN TRAINING LOOP\n",
    "# ==========================================\n",
    "def train_one_run(baseline, env_name, seed, steps_total):\n",
    "    set_global_seed(seed)\n",
    "    label = f\"{baseline}|{env_name}|{seed}|{MODEL_TAG}\"\n",
    "    safe_base = baseline.replace(\" \", \"_\")\n",
    "    safe_env = env_name.replace(\" \", \"_\")\n",
    "    run_dir = os.path.join(RUNS_DIR, safe_base, safe_env, f\"seed_{seed}\")\n",
    "    ckpt_dir = os.path.join(run_dir, \"checkpoints\")\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    train_env = make_train_env(baseline, LAYOUT, env_name)\n",
    "    eval_env = make_env(env_name, LAYOUT)\n",
    "\n",
    "    final_path = os.path.join(run_dir, \"final_model.zip\")\n",
    "    if os.path.exists(final_path):\n",
    "        print(f\"Skipping {label}, already has final_model.zip\")\n",
    "        return\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Robust checkpoint resume logic\n",
    "    # -------------------------------------------------\n",
    "    agent = None\n",
    "    reset_flag = True  # default to fresh run\n",
    "\n",
    "    if os.path.isdir(ckpt_dir):\n",
    "        cands = [f for f in os.listdir(ckpt_dir) if f.startswith(\"ppo_\") and f.endswith(\".zip\")]\n",
    "        if cands:\n",
    "            # Try newest checkpoints first\n",
    "            cands.sort(key=lambda x: int(re.search(r\"(\\d+)\", x).group(1)), reverse=True)\n",
    "            for fname in cands:\n",
    "                ckpt_path = os.path.join(ckpt_dir, fname)\n",
    "                try:\n",
    "                    print(f\"Trying checkpoint {ckpt_path} for {label}\")\n",
    "                    agent = PPO.load(ckpt_path, env=train_env, device=DEVICE, verbose=1)\n",
    "                    reset_flag = False\n",
    "                    print(f\"Resumed {label} from {ckpt_path}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load checkpoint {ckpt_path}: {e}. Removing it.\")\n",
    "                    try:\n",
    "                        os.remove(ckpt_path)\n",
    "                    except OSError:\n",
    "                        pass\n",
    "\n",
    "    if agent is None:\n",
    "        print(f\"Starting fresh run {label}\")\n",
    "        agent = PPO(\n",
    "            \"MlpPolicy\",\n",
    "            train_env,\n",
    "            n_steps=2048,\n",
    "            batch_size=2048,\n",
    "            learning_rate=3e-4,\n",
    "            gamma=0.99,\n",
    "            verbose=1,\n",
    "            seed=seed,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        reset_flag = True\n",
    "\n",
    "    callbacks = [\n",
    "        CheckpointCallback(save_freq=CHECKPOINT_EVERY_STEPS, save_path=ckpt_dir, name_prefix=\"ppo\"),\n",
    "        StopTrainingOnMaxSteps(max_steps=steps_total, verbose=1),\n",
    "    ]\n",
    "\n",
    "    t0 = time.time()\n",
    "    agent.learn(\n",
    "        total_timesteps=steps_total,\n",
    "        callback=callbacks,\n",
    "        reset_num_timesteps=reset_flag,\n",
    "    )\n",
    "    minutes = (time.time() - t0) / 60.0\n",
    "\n",
    "    agent.save(final_path)\n",
    "    with open(os.path.join(run_dir, \"meta.json\"), \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"baseline\": baseline,\n",
    "                \"env\": env_name,\n",
    "                \"seed\": seed,\n",
    "                \"steps\": steps_total,\n",
    "                \"model_tag\": MODEL_TAG,\n",
    "                \"llm_model\": LLM_MODEL_NAME,\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "\n",
    "    mean, std = evaluate(agent, eval_env, episodes=EVAL_EPISODES)\n",
    "    with open(RESULTS_CSV, \"a\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow(\n",
    "            [\n",
    "                baseline,\n",
    "                env_name,\n",
    "                seed,\n",
    "                \"final\",\n",
    "                round(mean, 4),\n",
    "                round(std, 4),\n",
    "                round(minutes, 3),\n",
    "                MODEL_TAG,\n",
    "                LLM_MODEL_NAME,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    print(f\"âœ… [{label}] FINISH in {minutes:.2f} min | eval {mean:.2f}Â±{std:.2f}\")\n",
    "    return (baseline, env_name, seed, mean)\n",
    "\n",
    "# ==========================================\n",
    "# 7. PARALLEL EXECUTION ENTRY POINT\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize CSV\n",
    "    if not os.path.exists(RESULTS_CSV):\n",
    "        with open(RESULTS_CSV, \"w\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(\n",
    "                [\n",
    "                    \"baseline\",\n",
    "                    \"env\",\n",
    "                    \"seed\",\n",
    "                    \"phase\",\n",
    "                    \"mean_return\",\n",
    "                    \"std_dev\",\n",
    "                    \"train_minutes\",\n",
    "                    \"model_tag\",\n",
    "                    \"llm_model\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    baselines = [\"PPO+LLM\"]\n",
    "    env_names = [\"No Noise\", \"Noise\", \"Delay\", \"Combo\"]\n",
    "\n",
    "    all_jobs = []\n",
    "    for b in baselines:\n",
    "        steps = BASELINE_STEPS.get(b, 600_000)\n",
    "        for e in env_names:\n",
    "            for s in SEEDS:\n",
    "                all_jobs.append((b, e, s, steps))\n",
    "\n",
    "    print(f\"ðŸš€ Launching PPO+LLM jobs for {MODEL_TAG} over {len(env_names)} envs and {len(SEEDS)} seeds\")\n",
    "    print(f\"Total runs: {len(all_jobs)}\")  # should print 20\n",
    "\n",
    "    Parallel(n_jobs=20)(\n",
    "        delayed(train_one_run)(b, e, s, steps)\n",
    "        for b, e, s, steps in all_jobs\n",
    "    )\n",
    "\n",
    "    print(\"\\nðŸŽ‰ ALL RUNS COMPLETE FOR GPT-NEO 125M.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
