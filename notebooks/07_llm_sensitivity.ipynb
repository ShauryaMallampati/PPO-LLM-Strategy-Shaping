{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f112307a",
   "metadata": {},
   "source": [
    "# üß™ GPT-Neo 125M Sensitivity Analysis\n",
    "\n",
    "**Compare smaller vs larger LLM for reward shaping.**\n",
    "\n",
    "This notebook tests whether a smaller, faster LLM (GPT-Neo 125M) can achieve\n",
    "similar reward shaping benefits compared to GPT-Neo 1.3B.\n",
    "\n",
    "## Research Questions:\n",
    "- Does model size affect shaping quality?\n",
    "- Can we get the benefits of LLM shaping with reduced compute?\n",
    "- What's the latency vs performance tradeoff?\n",
    "\n",
    "‚ö†Ô∏è **GPU Required** for reasonable inference times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39922d8f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab)\n",
    "!pip install stable-baselines3 gymnasium 'shimmy>=1.3.0' transformers torch matplotlib pandas tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00688199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and install Overcooked-AI\n",
    "!git clone https://github.com/HumanCompatibleAI/overcooked_ai.git\n",
    "%cd overcooked_ai\n",
    "!pip install -e . -q\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb02cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gymnasium import Wrapper\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbbfd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a35519",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe261bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# Models to compare\n",
    "MODELS = {\n",
    "    \"GPT-Neo-125M\": \"EleutherAI/gpt-neo-125m\",\n",
    "    \"GPT-Neo-1.3B\": \"EleutherAI/gpt-neo-1.3B\",\n",
    "}\n",
    "\n",
    "# Environment settings\n",
    "LAYOUT = \"asymmetric_advantages\"\n",
    "HORIZON = 400\n",
    "\n",
    "# Training settings\n",
    "TIMESTEPS = 100_000  # Shorter runs for sensitivity analysis\n",
    "SEEDS = [42, 123, 456]\n",
    "\n",
    "# LLM shaping\n",
    "SHAPING_COEFF = 0.05\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/llm_sensitivity\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Testing models: {list(MODELS.keys())}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Seeds: {SEEDS}\")\n",
    "print(f\"Timesteps: {TIMESTEPS:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec369e",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ LLM Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2815ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm(model_name, model_path):\n",
    "    \"\"\"Load an LLM model and tokenizer.\"\"\"\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "    ).to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Count parameters\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Parameters: {n_params / 1e6:.1f}M\")\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both models\n",
    "loaded_models = {}\n",
    "for name, path in MODELS.items():\n",
    "    model, tokenizer = load_llm(name, path)\n",
    "    loaded_models[name] = {\"model\": model, \"tokenizer\": tokenizer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2391f56",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd12f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_latency(model, tokenizer, n_samples=100):\n",
    "    \"\"\"Measure average inference latency.\"\"\"\n",
    "    \n",
    "    test_prompt = (\n",
    "        \"In a cooperative cooking game, evaluate this state: \"\n",
    "        \"Player 1 is holding a tomato near the pot. \"\n",
    "        \"Player 2 is at the dish dispenser.\"\n",
    "    )\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        inputs = tokenizer(test_prompt, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    # Measure\n",
    "    latencies = []\n",
    "    for _ in range(n_samples):\n",
    "        inputs = tokenizer(test_prompt, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        \n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "        \n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        latencies.append((time.perf_counter() - start) * 1000)  # ms\n",
    "    \n",
    "    return np.mean(latencies), np.std(latencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare inference latency\n",
    "print(\"Measuring inference latency...\")\n",
    "latency_results = []\n",
    "\n",
    "for name, data in loaded_models.items():\n",
    "    mean_ms, std_ms = measure_inference_latency(data[\"model\"], data[\"tokenizer\"])\n",
    "    latency_results.append({\n",
    "        \"model\": name,\n",
    "        \"latency_ms\": mean_ms,\n",
    "        \"latency_std\": std_ms\n",
    "    })\n",
    "    print(f\"  {name}: {mean_ms:.2f} ¬± {std_ms:.2f} ms\")\n",
    "\n",
    "latency_df = pd.DataFrame(latency_results)\n",
    "latency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latency comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars = ax.bar(latency_df[\"model\"], latency_df[\"latency_ms\"], \n",
    "              yerr=latency_df[\"latency_std\"], capsize=5, color=[\"#3498db\", \"#e74c3c\"])\n",
    "ax.set_ylabel(\"Latency (ms)\")\n",
    "ax.set_title(\"LLM Inference Latency Comparison\")\n",
    "\n",
    "# Add speedup annotation\n",
    "if len(latency_df) == 2:\n",
    "    speedup = latency_df.iloc[1][\"latency_ms\"] / latency_df.iloc[0][\"latency_ms\"]\n",
    "    ax.annotate(f\"{speedup:.1f}x faster\", xy=(0, latency_df.iloc[0][\"latency_ms\"]),\n",
    "                xytext=(0.3, latency_df.iloc[1][\"latency_ms\"] * 0.8),\n",
    "                fontsize=12, color=\"green\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"latency_comparison.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d5225",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Environment Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoAgentOvercookedEnv(gym.Env):\n",
    "    \"\"\"Gym wrapper for Overcooked with joint action space.\"\"\"\n",
    "    \n",
    "    def __init__(self, layout=\"asymmetric_advantages\", horizon=400):\n",
    "        super().__init__()\n",
    "        self.mdp = OvercookedGridworld.from_layout_name(layout)\n",
    "        self.base_env = OvercookedEnv.from_mdp(self.mdp, horizon=horizon)\n",
    "        self.horizon = horizon\n",
    "        \n",
    "        n_act = 6  # per agent\n",
    "        obs_dim = self.base_env.featurize_state_mdp(self.base_env.state)[0].shape[0]\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(n_act * n_act)  # joint\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(2 * obs_dim,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        self.base_env.reset()\n",
    "        obs = self._get_obs()\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, joint_action):\n",
    "        a0 = joint_action // 6\n",
    "        a1 = joint_action % 6\n",
    "        next_state, reward, done, info = self.base_env.step([a0, a1])\n",
    "        obs = self._get_obs()\n",
    "        return obs, float(reward), done, False, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        f0, f1 = self.base_env.featurize_state_mdp(self.base_env.state)\n",
    "        return np.concatenate([f0, f1]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26481ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMShapingWrapper(Wrapper):\n",
    "    \"\"\"Adds LLM-based reward shaping.\"\"\"\n",
    "    \n",
    "    def __init__(self, env, model, tokenizer, coeff=0.05):\n",
    "        super().__init__(env)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.coeff = coeff\n",
    "        self.total_shaped_reward = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Compute LLM shaping bonus\n",
    "        shaped = self._compute_shaping(obs)\n",
    "        self.total_shaped_reward += shaped\n",
    "        \n",
    "        return obs, reward + shaped, done, truncated, info\n",
    "\n",
    "    def _compute_shaping(self, obs):\n",
    "        \"\"\"Get LLM-based reward shaping.\"\"\"\n",
    "        prompt = f\"State features: {obs[:10].tolist()}\"  # Truncated for speed\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt, return_tensors=\"pt\", truncation=True, max_length=128\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Convert to scalar bonus\n",
    "        bonus = torch.tanh(logits.mean()).item()\n",
    "        return self.coeff * bonus\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.total_shaped_reward = 0\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125bb50",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Training Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35c78d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_llm(model_name, model, tokenizer, seed, timesteps=100000):\n",
    "    \"\"\"Train PPO with LLM shaping.\"\"\"\n",
    "    \n",
    "    # Create environment with shaping\n",
    "    env = TwoAgentOvercookedEnv(LAYOUT, HORIZON)\n",
    "    env = LLMShapingWrapper(env, model, tokenizer, SHAPING_COEFF)\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    # Train\n",
    "    agent = PPO(\"MlpPolicy\", vec_env, seed=seed, verbose=0)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    agent.learn(total_timesteps=timesteps)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate on base env (no shaping)\n",
    "    eval_env = TwoAgentOvercookedEnv(LAYOUT, HORIZON)\n",
    "    eval_vec = DummyVecEnv([lambda: eval_env])\n",
    "    mean_reward, std_reward = evaluate_policy(agent, eval_vec, n_eval_episodes=10)\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"seed\": seed,\n",
    "        \"mean_reward\": mean_reward,\n",
    "        \"std_reward\": std_reward,\n",
    "        \"train_time_s\": train_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158aadaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training comparison\n",
    "results = []\n",
    "\n",
    "for model_name, data in tqdm(loaded_models.items(), desc=\"Models\"):\n",
    "    for seed in tqdm(SEEDS, desc=f\"{model_name} seeds\", leave=False):\n",
    "        result = train_with_llm(\n",
    "            model_name, \n",
    "            data[\"model\"], \n",
    "            data[\"tokenizer\"], \n",
    "            seed, \n",
    "            TIMESTEPS\n",
    "        )\n",
    "        results.append(result)\n",
    "        print(f\"  {model_name} seed={seed}: {result['mean_reward']:.2f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(OUTPUT_DIR, \"sensitivity_results.csv\"), index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd22bb8a",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "agg = results_df.groupby(\"model\").agg({\n",
    "    \"mean_reward\": [\"mean\", \"std\"],\n",
    "    \"train_time_s\": [\"mean\", \"std\"]\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nüìä Aggregated Results:\")\n",
    "display(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Reward comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Reward comparison\n",
    "ax1 = axes[0]\n",
    "model_means = results_df.groupby(\"model\")[\"mean_reward\"].mean()\n",
    "model_stds = results_df.groupby(\"model\")[\"mean_reward\"].std()\n",
    "ax1.bar(model_means.index, model_means.values, yerr=model_stds.values, \n",
    "        capsize=5, color=[\"#3498db\", \"#e74c3c\"])\n",
    "ax1.set_ylabel(\"Mean Reward\")\n",
    "ax1.set_title(\"Training Performance by LLM Size\")\n",
    "\n",
    "# Training time comparison\n",
    "ax2 = axes[1]\n",
    "time_means = results_df.groupby(\"model\")[\"train_time_s\"].mean()\n",
    "time_stds = results_df.groupby(\"model\")[\"train_time_s\"].std()\n",
    "ax2.bar(time_means.index, time_means.values / 60, yerr=time_stds.values / 60, \n",
    "        capsize=5, color=[\"#3498db\", \"#e74c3c\"])\n",
    "ax2.set_ylabel(\"Training Time (minutes)\")\n",
    "ax2.set_title(\"Training Time by LLM Size\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"model_comparison.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency metric: reward per training minute\n",
    "results_df[\"reward_per_minute\"] = results_df[\"mean_reward\"] / (results_df[\"train_time_s\"] / 60)\n",
    "\n",
    "efficiency = results_df.groupby(\"model\")[\"reward_per_minute\"].mean()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(efficiency.index, efficiency.values, color=[\"#3498db\", \"#e74c3c\"])\n",
    "plt.ylabel(\"Reward per Training Minute\")\n",
    "plt.title(\"Training Efficiency by LLM Size\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"efficiency_comparison.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7223e17",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ecb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined summary\n",
    "summary = latency_df.merge(\n",
    "    results_df.groupby(\"model\").agg({\n",
    "        \"mean_reward\": \"mean\",\n",
    "        \"train_time_s\": \"mean\"\n",
    "    }).reset_index(),\n",
    "    on=\"model\"\n",
    ")\n",
    "\n",
    "summary[\"train_time_min\"] = summary[\"train_time_s\"] / 60\n",
    "summary = summary.drop(columns=[\"latency_std\", \"train_time_s\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà LLM SIZE SENSITIVITY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "display(summary.round(2))\n",
    "\n",
    "# Key findings\n",
    "print(\"\\nüîç Key Findings:\")\n",
    "if len(summary) == 2:\n",
    "    small = summary[summary[\"model\"] == \"GPT-Neo-125M\"].iloc[0]\n",
    "    large = summary[summary[\"model\"] == \"GPT-Neo-1.3B\"].iloc[0]\n",
    "    \n",
    "    latency_speedup = large[\"latency_ms\"] / small[\"latency_ms\"]\n",
    "    reward_diff = ((small[\"mean_reward\"] - large[\"mean_reward\"]) / abs(large[\"mean_reward\"])) * 100\n",
    "    time_speedup = large[\"train_time_min\"] / small[\"train_time_min\"]\n",
    "    \n",
    "    print(f\"  ‚Ä¢ 125M is {latency_speedup:.1f}x faster per inference\")\n",
    "    print(f\"  ‚Ä¢ Reward difference: {reward_diff:+.1f}%\")\n",
    "    print(f\"  ‚Ä¢ Training speedup: {time_speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08167f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final summary\n",
    "summary.to_csv(os.path.join(OUTPUT_DIR, \"final_summary.csv\"), index=False)\n",
    "print(f\"\\n‚úÖ All results saved to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
