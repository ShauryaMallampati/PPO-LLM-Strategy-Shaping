{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ad98d9",
   "metadata": {},
   "source": [
    "# 06 - Task Completion Analysis\n",
    "\n",
    "Computes sparse reward task completion metrics.\n",
    "\n",
    "**Output:**\n",
    "- `/content/drive/MyDrive/task_completion_per_seed.csv`\n",
    "- `/content/drive/MyDrive/task_completion_agg.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add792f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Script B: Sparse Reward Task Completion\n",
    "# ============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to your training results CSV (adjust if needed)\n",
    "RESULTS_CSV = \"/content/drive/MyDrive/results_combined_new.csv\"\n",
    "\n",
    "# Output paths\n",
    "TASK_PER_SEED = \"/content/drive/MyDrive/task_completion_per_seed.csv\"\n",
    "TASK_AGG      = \"/content/drive/MyDrive/task_completion_agg.csv\"\n",
    "\n",
    "# Penalty floor for the sparse reward regime.\n",
    "# For asymmetric_advantages this is around -40 in your runs.\n",
    "# You can tweak this if you want a different floor.\n",
    "R_MIN = -40.0\n",
    "\n",
    "print(f\"Loading training results from: {RESULTS_CSV}\")\n",
    "df = pd.read_csv(RESULTS_CSV)\n",
    "\n",
    "# Only use final policies\n",
    "df_final = df[df[\"phase\"] == \"final\"].copy()\n",
    "\n",
    "expected_cols = {\n",
    "    \"baseline\", \"env\", \"seed\",\n",
    "    \"phase\", \"mean_return\", \"std_dev\", \"train_minutes\"\n",
    "}\n",
    "missing = expected_cols - set(df_final.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV is missing columns: {missing}\")\n",
    "\n",
    "# Clean env labels a bit\n",
    "df_final[\"env_clean\"] = df_final[\"env\"].str.strip()\n",
    "\n",
    "# One row per (baseline, env, seed)\n",
    "grouped = (\n",
    "    df_final\n",
    "    .groupby([\"baseline\", \"env_clean\", \"seed\"], as_index=False)\n",
    "    .agg(\n",
    "        mean_return=(\"mean_return\", \"mean\"),\n",
    "        std_return=(\"mean_return\", \"std\"),   # across any dup rows (usually just 1)\n",
    "        mean_std_dev=(\"std_dev\", \"mean\"),    # mean of per-run episode stds\n",
    "    )\n",
    ")\n",
    "\n",
    "# ----- Completion metrics -----\n",
    "# completion_score = distance above penalty floor\n",
    "grouped[\"completion_score\"] = grouped[\"mean_return\"] - R_MIN\n",
    "# normalized to approx [0, 1] for R in [R_MIN, 0]\n",
    "grouped[\"completion_norm\"]  = grouped[\"completion_score\"] / abs(R_MIN)\n",
    "\n",
    "# Save per-seed metrics\n",
    "grouped.to_csv(TASK_PER_SEED, index=False)\n",
    "print(f\"Saved per-seed task completion metrics to: {TASK_PER_SEED}\")\n",
    "\n",
    "# ----- Aggregate over seeds per (baseline, env) -----\n",
    "agg = (\n",
    "    grouped\n",
    "    .groupby([\"baseline\", \"env_clean\"], as_index=False)\n",
    "    .agg(\n",
    "        mean_return_mean=(\"mean_return\", \"mean\"),\n",
    "        mean_return_std=(\"mean_return\", \"std\"),\n",
    "\n",
    "        completion_score_mean=(\"completion_score\", \"mean\"),\n",
    "        completion_score_std=(\"completion_score\", \"std\"),\n",
    "\n",
    "        completion_norm_mean=(\"completion_norm\", \"mean\"),\n",
    "        completion_norm_std=(\"completion_norm\", \"std\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "agg.to_csv(TASK_AGG, index=False)\n",
    "print(f\"Saved aggregated task completion metrics to: {TASK_AGG}\")\n",
    "\n",
    "print(\"\\nPreview (aggregated task-completion stats):\")\n",
    "print(agg.sort_values([\"env_clean\", \"baseline\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
