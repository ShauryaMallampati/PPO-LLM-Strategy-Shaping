{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a00992",
   "metadata": {},
   "source": [
    "# ‚úÖ Task Completion Analysis\n",
    "\n",
    "**Evaluate actual task performance** in the sparse reward Overcooked environment.\n",
    "\n",
    "## What this measures:\n",
    "- **Completion Score** = Mean Return - Minimum Penalty\n",
    "- **Completion Norm** = Normalized score (0 = worst, 1 = best)\n",
    "- Proxy for \"dishes served\" in Overcooked\n",
    "\n",
    "‚ö†Ô∏è **Requires training results CSV from 02_training.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22566ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "print(\"Imports loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4a3eda",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbbf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# Path to your training results CSV\n",
    "RESULTS_CSV = \"/content/drive/MyDrive/results_combined_new.csv\"\n",
    "\n",
    "# Output paths\n",
    "TASK_PER_SEED = \"/content/drive/MyDrive/task_completion_per_seed.csv\"\n",
    "TASK_AGG = \"/content/drive/MyDrive/task_completion_agg.csv\"\n",
    "\n",
    "# Penalty floor for the sparse reward regime\n",
    "# For asymmetric_advantages this is around -40\n",
    "R_MIN = -40.0\n",
    "\n",
    "print(f\"Loading data from: {RESULTS_CSV}\")\n",
    "print(f\"Per-seed output: {TASK_PER_SEED}\")\n",
    "print(f\"Aggregated output: {TASK_AGG}\")\n",
    "print(f\"Reward floor: {R_MIN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa054859",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e7e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading training results from: {RESULTS_CSV}\")\n",
    "df = pd.read_csv(RESULTS_CSV)\n",
    "\n",
    "# Only use final policies\n",
    "df_final = df[df[\"phase\"] == \"final\"].copy()\n",
    "\n",
    "expected_cols = {\n",
    "    \"baseline\", \"env\", \"seed\",\n",
    "    \"phase\", \"mean_return\", \"std_dev\", \"train_minutes\"\n",
    "}\n",
    "missing = expected_cols - set(df_final.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV is missing columns: {missing}\")\n",
    "\n",
    "print(f\"Loaded {len(df_final)} final results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean env labels\n",
    "df_final[\"env_clean\"] = df_final[\"env\"].str.strip()\n",
    "\n",
    "# One row per (baseline, env, seed)\n",
    "grouped = (\n",
    "    df_final\n",
    "    .groupby([\"baseline\", \"env_clean\", \"seed\"], as_index=False)\n",
    "    .agg(\n",
    "        mean_return=(\"mean_return\", \"mean\"),\n",
    "        std_return=(\"mean_return\", \"std\"),\n",
    "        mean_std_dev=(\"std_dev\", \"mean\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Grouped into {len(grouped)} unique configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7793da",
   "metadata": {},
   "source": [
    "## Compute Completion Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641eda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completion metrics\n",
    "# completion_score = distance above penalty floor\n",
    "grouped[\"completion_score\"] = grouped[\"mean_return\"] - R_MIN\n",
    "\n",
    "# normalized to approx [0, 1] for R in [R_MIN, 0]\n",
    "grouped[\"completion_norm\"] = grouped[\"completion_score\"] / abs(R_MIN)\n",
    "\n",
    "print(\"Completion metrics computed!\")\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9f6bc",
   "metadata": {},
   "source": [
    "## Save Per-Seed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941670fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save per-seed metrics\n",
    "grouped.to_csv(TASK_PER_SEED, index=False)\n",
    "print(f\"‚úÖ Saved per-seed task completion metrics to: {TASK_PER_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af16cde",
   "metadata": {},
   "source": [
    "## Aggregate Over Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b52352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate over seeds per (baseline, env)\n",
    "agg = (\n",
    "    grouped\n",
    "    .groupby([\"baseline\", \"env_clean\"], as_index=False)\n",
    "    .agg(\n",
    "        mean_return_mean=(\"mean_return\", \"mean\"),\n",
    "        mean_return_std=(\"mean_return\", \"std\"),\n",
    "\n",
    "        completion_score_mean=(\"completion_score\", \"mean\"),\n",
    "        completion_score_std=(\"completion_score\", \"std\"),\n",
    "\n",
    "        completion_norm_mean=(\"completion_norm\", \"mean\"),\n",
    "        completion_norm_std=(\"completion_norm\", \"std\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "agg.to_csv(TASK_AGG, index=False)\n",
    "print(f\"‚úÖ Saved aggregated task completion metrics to: {TASK_AGG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a2008",
   "metadata": {},
   "source": [
    "## üìä View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd98050",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Aggregated Task Completion Stats:\")\n",
    "display(agg.sort_values([\"env_clean\", \"baseline\"]).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7961eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by baseline (averaged across envs)\n",
    "baseline_summary = agg.groupby(\"baseline\").agg({\n",
    "    \"mean_return_mean\": \"mean\",\n",
    "    \"completion_score_mean\": \"mean\",\n",
    "    \"completion_norm_mean\": \"mean\",\n",
    "}).round(2).sort_values(\"completion_norm_mean\", ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ Baseline Ranking (by completion norm):\")\n",
    "display(baseline_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e44af3",
   "metadata": {},
   "source": [
    "## üìà Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart by baseline and environment\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(data=agg, x=\"baseline\", y=\"completion_score_mean\", hue=\"env_clean\")\n",
    "plt.title(\"Task Completion Score by Baseline and Environment\")\n",
    "plt.xlabel(\"Baseline\")\n",
    "plt.ylabel(\"Completion Score (higher = better)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Environment\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca35fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of normalized completion\n",
    "pivot = agg.pivot(index=\"baseline\", columns=\"env_clean\", values=\"completion_norm_mean\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", vmin=0, vmax=1)\n",
    "plt.title(\"Normalized Task Completion (0 = worst, 1 = best)\")\n",
    "plt.xlabel(\"Environment\")\n",
    "plt.ylabel(\"Baseline\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53eab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PPO+LLM vs others\n",
    "ppo_llm = agg[agg[\"baseline\"] == \"PPO+LLM\"].set_index(\"env_clean\")[\"completion_norm_mean\"]\n",
    "baseline = agg[agg[\"baseline\"] == \"Baseline\"].set_index(\"env_clean\")[\"completion_norm_mean\"]\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"PPO+LLM\": ppo_llm,\n",
    "    \"Baseline\": baseline,\n",
    "    \"Improvement\": ppo_llm - baseline\n",
    "})\n",
    "\n",
    "print(\"\\nüîç PPO+LLM vs Baseline Comparison:\")\n",
    "display(comparison.round(3))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
