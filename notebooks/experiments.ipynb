{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b626898",
   "metadata": {},
   "source": [
    "# PPO-LLM Strategy Shaping Experiments\n",
    "\n",
    "This notebook provides an interactive interface for running and analyzing the PPO-LLM Strategy Shaping experiments.\n",
    "\n",
    "## Contents\n",
    "1. Setup & Configuration\n",
    "2. Training All Baselines\n",
    "3. Nash Gap Analysis\n",
    "4. Latency Measurement\n",
    "5. Robustness Analysis\n",
    "6. Task Completion Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe6f8b",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install package if needed (uncomment if running for first time)\n",
    "# !pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5747dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from ppo_llm_strategy_shaping import (\n",
    "    Config,\n",
    "    train_all,\n",
    "    train_one_run,\n",
    "    load_or_train,\n",
    "    make_env,\n",
    "    evaluate,\n",
    "    nash_gap_analysis,\n",
    "    latency_analysis,\n",
    "    robustness_analysis,\n",
    "    task_completion_analysis,\n",
    "    generate_summary_report,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiment\n",
    "config = Config(\n",
    "    layout=\"cramped_room\",\n",
    "    horizon=400,\n",
    "    seeds=[42, 123, 456, 789, 1000],\n",
    "    \n",
    "    # PPO hyperparameters\n",
    "    lr=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    \n",
    "    # LLM settings\n",
    "    llm_model=\"EleutherAI/gpt-neo-125M\",  # Use smaller model for faster experiments\n",
    "    llm_reward_scale=0.1,\n",
    "    \n",
    "    # Perturbation settings\n",
    "    noise_std=0.01,\n",
    "    delay_prob=0.2,\n",
    "    delay_penalty=-0.5,\n",
    ")\n",
    "\n",
    "print(f\"Layout: {config.layout}\")\n",
    "print(f\"Horizon: {config.horizon}\")\n",
    "print(f\"Seeds: {config.seeds}\")\n",
    "print(f\"Baselines: {list(config.baseline_steps.keys())}\")\n",
    "print(f\"Environments: {config.all_envs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca914c49",
   "metadata": {},
   "source": [
    "## 2. Training All Baselines\n",
    "\n",
    "Train all baseline methods across all environment perturbations and seeds.\n",
    "\n",
    "**Note**: Full training takes several hours. For quick testing, reduce seeds or use a subset of baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: train single configuration\n",
    "# model, run_dir, train_time = train_one_run(\n",
    "#     config,\n",
    "#     baseline=\"PPO+LLM\",\n",
    "#     env_name=\"No Noise\",\n",
    "#     seed=42,\n",
    "#     verbose=1\n",
    "# )\n",
    "# print(f\"Training completed in {train_time:.1f}s\")\n",
    "# print(f\"Model saved to: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training (uncomment to run)\n",
    "# WARNING: This will take many hours!\n",
    "\n",
    "# results = train_all(\n",
    "#     config,\n",
    "#     baselines=[\"Baseline\", \"PPO+LLM\"],  # Start with subset\n",
    "#     env_names=[\"No Noise\", \"Noise\"],\n",
    "#     seeds=[42, 123],\n",
    "#     n_jobs=4,  # Parallel workers\n",
    "#     verbose=1\n",
    "# )\n",
    "# print(f\"Completed {len(results)} training runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161f0e8",
   "metadata": {},
   "source": [
    "## 3. Nash Gap Analysis\n",
    "\n",
    "Compute Nash gaps to measure how close trained policies are to Nash equilibrium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Nash gap analysis (requires trained models)\n",
    "# nash_results = nash_gap_analysis(\n",
    "#     config,\n",
    "#     n_episodes=5,\n",
    "#     output_csv=config.results_root + \"/nash_gap.csv\"\n",
    "# )\n",
    "\n",
    "# # Display results\n",
    "# for baseline, envs in nash_results.items():\n",
    "#     print(f\"\\n{baseline}:\")\n",
    "#     for env_name, gap in envs.items():\n",
    "#         print(f\"  {env_name}: {gap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96879862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Nash gaps\n",
    "# df = pd.read_csv(config.results_root + \"/nash_gap.csv\")\n",
    "# \n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.barplot(data=df, x=\"baseline\", y=\"nash_gap\", hue=\"env\")\n",
    "# plt.title(\"Nash Gap by Baseline and Environment\")\n",
    "# plt.ylabel(\"Nash Gap (lower is better)\")\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14743f23",
   "metadata": {},
   "source": [
    "## 4. Latency Measurement\n",
    "\n",
    "Measure inference latency for each baseline to assess real-time deployment feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run latency analysis (requires trained models)\n",
    "# latency_results = latency_analysis(\n",
    "#     config,\n",
    "#     n_steps=1000,\n",
    "#     output_csv=config.results_root + \"/latency.csv\"\n",
    "# )\n",
    "\n",
    "# # Display results\n",
    "# for baseline, latency in latency_results.items():\n",
    "#     print(f\"{baseline}: {latency['mean_ms']:.2f}ms (p95: {latency['p95_ms']:.2f}ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latency\n",
    "# df = pd.read_csv(config.results_root + \"/latency.csv\")\n",
    "# \n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# \n",
    "# axes[0].bar(df['baseline'], df['mean_ms'], yerr=df['std_ms'], capsize=5)\n",
    "# axes[0].set_title(\"Mean Inference Latency\")\n",
    "# axes[0].set_ylabel(\"Latency (ms)\")\n",
    "# axes[0].tick_params(axis='x', rotation=45)\n",
    "# \n",
    "# axes[1].bar(df['baseline'], df['p95_ms'])\n",
    "# axes[1].set_title(\"95th Percentile Latency\")\n",
    "# axes[1].set_ylabel(\"Latency (ms)\")\n",
    "# axes[1].tick_params(axis='x', rotation=45)\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cb99b",
   "metadata": {},
   "source": [
    "## 5. Robustness Analysis\n",
    "\n",
    "Test how well policies trained in one environment generalize to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f694e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run robustness analysis (requires trained models)\n",
    "# robustness_results = robustness_analysis(\n",
    "#     config,\n",
    "#     n_episodes=10,\n",
    "#     output_csv=config.results_root + \"/robustness.csv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d860a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize robustness as heatmaps\n",
    "# df = pd.read_csv(config.results_root + \"/robustness.csv\")\n",
    "# \n",
    "# baselines_to_plot = [\"Baseline\", \"PPO+LLM\"]\n",
    "# \n",
    "# fig, axes = plt.subplots(1, len(baselines_to_plot), figsize=(6*len(baselines_to_plot), 5))\n",
    "# \n",
    "# for ax, baseline in zip(axes, baselines_to_plot):\n",
    "#     subset = df[df['baseline'] == baseline].groupby(['train_env', 'test_env'])['mean_reward'].mean().unstack()\n",
    "#     sns.heatmap(subset, annot=True, fmt='.1f', cmap='YlGnBu', ax=ax)\n",
    "#     ax.set_title(f\"{baseline} Robustness\")\n",
    "#     ax.set_xlabel(\"Test Environment\")\n",
    "#     ax.set_ylabel(\"Train Environment\")\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1fe25",
   "metadata": {},
   "source": [
    "## 6. Task Completion Metrics\n",
    "\n",
    "Measure actual task performance (dishes served) in Overcooked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run task completion analysis (requires trained models)\n",
    "# task_results = task_completion_analysis(\n",
    "#     config,\n",
    "#     n_episodes=10,\n",
    "#     output_csv=config.results_root + \"/task_completion.csv\"\n",
    "# )\n",
    "\n",
    "# # Display results\n",
    "# for baseline, envs in task_results.items():\n",
    "#     print(f\"\\n{baseline}:\")\n",
    "#     for env_name, metrics in envs.items():\n",
    "#         print(f\"  {env_name}: {metrics['mean_dishes']:.2f} dishes (rate: {metrics['completion_rate']:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cded52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize task completion\n",
    "# df = pd.read_csv(config.results_root + \"/task_completion.csv\")\n",
    "# \n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.barplot(data=df, x=\"baseline\", y=\"mean_dishes\", hue=\"env\")\n",
    "# plt.title(\"Task Completion by Baseline and Environment\")\n",
    "# plt.ylabel(\"Mean Dishes Served\")\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91243155",
   "metadata": {},
   "source": [
    "## Summary Report\n",
    "\n",
    "Generate a comprehensive summary of all analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d113c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full summary report (requires all analyses to be run)\n",
    "# report = generate_summary_report(config)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f395f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Environment Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2344ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a single episode (requires trained model)\n",
    "# from ppo_llm_strategy_shaping import make_env\n",
    "# from stable_baselines3 import PPO\n",
    "# \n",
    "# env = make_env(config, \"PPO+LLM\", \"No Noise\", seed=42)\n",
    "# model = PPO.load(\"path/to/model.zip\", env=env)\n",
    "# \n",
    "# obs, _ = env.reset()\n",
    "# for _ in range(100):\n",
    "#     action, _ = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, truncated, info = env.step(action)\n",
    "#     # env.render()  # Uncomment if render is available\n",
    "#     if done or truncated:\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
